{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks as real-valued circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single Gate in Circuit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forwardMultiplyGate(x,y):\n",
    "    \"Takes two real-valued inputs and computes x * y with the * gate\"\n",
    "    \n",
    "    return x * y\n",
    "forwardMultiplyGate(-2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Circuit has been provided with specific input values (-2,3)\n",
    "\n",
    "2.The circuit computes an output value (-6)\n",
    "\n",
    "**3.How tweak input slightly to increase output ?**\n",
    "\n",
    "options :\n",
    "\n",
    "    a) Numerical gradient\n",
    "    b) Analytical gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical gradient (derivative)**\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial f(x,y)}{\\partial x} = \\frac{f(x+h,y )  - f(x,y)}{h}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative with respect to x is 3.000000000064062 \n",
      "Derivative with respect to y is -2.0000000000131024\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "x, y = -2, 3\n",
    "# infinitesimal\n",
    "h = 0.00001\n",
    "\n",
    "out = forwardMultiplyGate(x,y)\n",
    "\n",
    "#calculate derivate with respect to x\n",
    "xph = x + h\n",
    "\n",
    "out2 = forwardMultiplyGate(xph,y)\n",
    "\n",
    "x_derivative =  (out2 - out) / h\n",
    "\n",
    "#calculate derivative with respect to y\n",
    "yph = y + h\n",
    "\n",
    "out3 = forwardMultiplyGate(yph,x)\n",
    "\n",
    "y_derivative = (out3 - out) / h\n",
    "\n",
    "print('Derivative with respect to x is {0} \\nDerivative with respect to y is {1}'.format(x_derivative,y_derivative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change of inputs by derivative to  \n",
      "x = -1.9699999999993594, y = 2.979999999999869 \n",
      "leads to increased output\n",
      "output = -5.870599999997832\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "\n",
    "x +=  step_size * x_derivative\n",
    "\n",
    "y += step_size * y_derivative\n",
    "\n",
    "out_new = forwardMultiplyGate(x,y)\n",
    "\n",
    "print('Change of inputs by derivative to  \\nx = {}, y = {} \\nleads to increased output\\noutput = {}'.format(x,y,out_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical gradient is computationally expensive, because we need to compute circuit output as we tweak every input value independently by small amount.**\n",
    "\n",
    "**So complexity of evaluating gradient is linear to number of inputs**\n",
    "\n",
    "**Not practically usable to evaluate bigger and more mathematically complex networks**\n",
    "\n",
    "**Only gives approximations of gradient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical Gradient**\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f(x,y)}{\\partial x} = \\frac{f(x+h,y) - f(x,y)}{h}\n",
    "= \\frac{(x+h)y - xy}{h}\n",
    "= \\frac{xy + hy - xy}{h}\n",
    "= \\frac{hy}{h}\n",
    "= y\n",
    "\\end{equation}\n",
    "\n",
    "**Gives exact gradient**\n",
    "\n",
    "**Only one pass-through**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change of inputs by derivative to  \n",
      "x = -1.9699999999993594, y = 2.979999999999869 \n",
      "leads to increased output\n",
      "output = -5.870599999997832\n"
     ]
    }
   ],
   "source": [
    "x, y = -2, 3\n",
    "\n",
    "out = forwardMultiplyGate(x, y)\n",
    "\n",
    "x_gradient,y_gradient = y,x\n",
    "\n",
    "step_size = 0.01\n",
    "\n",
    "x +=  step_size * x_derivative\n",
    "\n",
    "y += step_size * y_derivative\n",
    "\n",
    "out_new = forwardMultiplyGate(x,y)\n",
    "\n",
    "print('Change of inputs by derivative to  \\nx = {}, y = {} \\nleads to increased output\\noutput = {}'.format(x,y,out_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In practice, NN libraries computes analytical gradient, but verifies implementation by comparing it to the numerical gradient**\n",
    "\n",
    "**Numerical gradient is expensive to compute, but easy to evaluate** \n",
    "\n",
    "vs. \n",
    "\n",
    "**Analytical gradient is efficient to compute, but may contain bugs at times**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recursive case : Circuits with multiple gates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardAddGate(a,b):\n",
    "    return a + b\n",
    "\n",
    "def forwardMultiplyGate(a,b):\n",
    "    return a * b"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "f(x,y,z) = (x + y) z\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "f(q,z) = q z \\hspace{0.5in} \\implies \\hspace{0.5in} \\frac{\\partial f(q,z)}{\\partial q} = z, \\hspace{1in} \\frac{\\partial f(q,z)}{\\partial z} = q\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "q(x,y) = x + y \\hspace{0.5in} \\implies \\hspace{0.5in} \\frac{\\partial q(x,y)}{\\partial x} = 1 \\hspace{1in} \\frac{\\partial q(x,y)}{\\partial y} = 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forwardCircuit(x,y,z):\n",
    "    \n",
    "    q = forwardAddGate(x,y)\n",
    "    f = forwardMultiplyGate(q,z)\n",
    "    \n",
    "    return f \n",
    "\n",
    "x, y, z = -2, 5, -4\n",
    "\n",
    "\n",
    "f = forwardCircuit(x,y,z)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation**\n",
    "\n",
    "- use chain rule to compute gradient with respect to x and y\n",
    "    - Chain Rule:\n",
    "\\begin{equation}\n",
    "f(x) = h(g(x))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "f'(x)= h'(g(x)) \\, g'(x)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4, -4)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial conditions\n",
    "\n",
    "x, y, z = -2, 5, -4\n",
    "\n",
    "q = forwardAddGate(x,y)\n",
    "\n",
    "f = forwardMultiplyGate(q,z)\n",
    "\n",
    "#gradient of the multiply gate with respect to its inputs\n",
    "\n",
    "derivative_f_wrt_z = q\n",
    "derivative_f_wrt_q = z\n",
    "\n",
    "#derivative of ADD gate with respect to its inputs\n",
    "\n",
    "derivative_q_wrt_x = 1\n",
    "derivative_q_wrt_y = 1\n",
    "\n",
    "#chain rule\n",
    "derivative_f_wrt_x = derivative_q_wrt_x * derivative_f_wrt_q\n",
    "derivative_f_wrt_y = derivative_q_wrt_y * derivative_f_wrt_q\n",
    "\n",
    "derivative_f_wrt_x,derivative_f_wrt_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial f(q,z)}{\\partial x} = \\frac{\\partial q(x,y)}{\\partial x} \\frac{\\partial f(q,z)}{\\partial q}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-4, -4, 3]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final gradient\n",
    "gradient_wtr_xyz = [derivative_f_wrt_x,derivative_f_wrt_y,derivative_f_wrt_z]\n",
    "gradient_wtr_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New inputs are x = -2.04(ov = 2),y = 4.96(ov = 5),z = -3.97(ov = -4) and theirs output is -11.5924\n"
     ]
    }
   ],
   "source": [
    "# let inputs respond to the force/tug\n",
    "step_size = 0.01\n",
    "\n",
    "x = x + step_size * derivative_f_wrt_x\n",
    "\n",
    "y = y + step_size * derivative_f_wrt_y\n",
    "\n",
    "z = z + step_size * derivative_f_wrt_z\n",
    "\n",
    "#Check if circuits improved\n",
    "q = forwardAddGate(x,y)\n",
    "\n",
    "f = forwardMultiplyGate(q,z)\n",
    "\n",
    "out_new = forwardCircuit(x,y,z)\n",
    "\n",
    "print('New inputs are x = {}(ov = 2),y = {}(ov = 5),z = {}(ov = -4) and theirs output is {}'.format(x,y,z,out_new))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAFjCAYAAADhM3UvAAAc+klEQVR4nO3dwXHiWhYGYAVAAqqadTfrtyEDQmD/NmSgFEjAC6fgFEiBCPpVEYKJ4cxinhiaFjbGQpx7+b4q1dTrcbvle04f/S0JqQkAAEbVPHoHAABqI2ABAIxMwAIAGJmABQAwMgELAGBkAhYAwMgELACAkQlYAAAjE7AAAEYmYAEAjEzAAgAYmYAFADAyAQsAYGQCFgDAyAQsAICRCVgAACMTsAAARiZgAQCMTMAiIiJ+/fplS7YBUC4Bi4iIaJomfvz4YUuyNY2/mgAlM8WJiHBAT0Y9AMpmihMRDujZqAdA2UxxIsIBPRv1ACibKU5EOKBnox4AZTPFiQgH9GzUA6BspjgR4YCejXoAlM0UJyIc0LNRD4CymeJEhAN6NuoBUDZTnIhwQM9GPQDKZooTEQ7o2agHQNlMcSLCAT0b9ZjGo983afMOTuplihMRDujZqMc0vIMz16bvqYluJiIc0LNRj2lY51zUg5roZiLCYMtGPaZhnXNRD2qimy84HA6xXq+jbdtomiaaponFYhHb7fbRu3YXBlsu6jEN65yLelAT3TzgcDjEfD4/hqqu62K9XsdsNoumaeL19fXRuzg6gy0X9ZiGdc5FPaiJbh7QdV00TRObzea3X9/tdjGbzWI2mz1oz+7HYMtFPaZhnXNRD2qimwesVqtomiYOh8Mf/996vY6maaq7VGiw5aIe07DOuagHNdHNX9Sf3RKwuCf1mIZ1zkU9qIlu/qL+pvehs1slM9hyUY9pWOdc1IOa6OYv2Gw20TRNrFarR+/K6Ay2XNRjGtY5F/WgJrr5Sq+vr9E0TbRtW93ZqwiDLRv1mIZ1zkU9qIluvkIfrmazWex2u0fvzl0YbLmoxzSscy7qQU2etpsXi8XxAaL91nXdH1/X39Tetu23wtX7+/vDX6L60Waw5aIe07DOuagHNXnabr4mYPWPa5jP59++LPjy8hI/f/5MuxlsuWSsx36/j9VqdXzgbtu2g/8oKUnGdX5m6kFNdPMFY4arEhhsuWSrx36/Pwar1WoVXdcd/5GyWCwevXs3y7bOz049qIluHvD29vZU4SrCYMsmWz36f3CcvyZquVxG0zTx9vb2oD37nmzr/OzUg5ro5gH9s67m83ksFovBrbab3Q22XLLVY7lcRtu2f/z6dru9eP9iCbKt87NTD2qim8/s9/s/7s0a2jzJnXsqpR79s+HO39tZilLW+VmoBzXRzUSEwZZN9nocDod4fX2N2WxW9LPhsq/zs1EPaqKbiQiDLZvM9egfXdJ/knC/3z96l26WeZ2fkXpQE91MRBhs2WSux2azia7rjje+l/wA3szr/IzUg5roZiLCYMumlHqcvkKqRKWs87NQD2qim4kIgy2bkuoxn8+jaZoiz2KVtM7PQD2oiW4mIgy2bEqqR//A0RI/WVvSOj8D9aAmupmIMNiyyVSPw+FwfCbckP65cSV+kjDTOqMe1EU3ExEGWzbZ6tGHqPOzVP1zsJbL5YP27HuyrfOzUw9qopuJCIMtm2z12O12F99FWPIrpbKt87NTD2qim4kIgy2bjPXY7XaxXC6PQatt21itVsWGq4ic6/zM1IOa6GYiwmDLRj2mYZ1zUQ9qopuJiPIHW//S4X4rXQ0/Qwmscy7qQU10MxFR/mATsLiFdc5FPaiJbiYiyh9sAha3sM65qAc10c1ERPmDTcDiFtY5F/WgJrqZiCh/sAlY3MI656Ie1EQ3ExHlDzYBi1tY51zUg5roZiKi/MEmYHEL65yLelAT3UxElD/YBCxuYZ1zUQ9qopuJiDIG22mA+u6WXdM08evXL9udtxJ64ZmoBzXRzUREGYPt2QLWz58/bXfeSuiFZ6Ie1EQ3ExFlDLZnC1jcn3XORT2oiW4mIsoYbNvt9uK22Wx+C1Affe12u330j/KpEupRA+uci3pQE91MRJQ/2Nzkzi2scy5Z67Hb7Y7/cBvy9vYWy+XyOH/ato31eh2Hw2HiPSWTnN3M5LIOtmsJWNzCOueSsR6HwyHm8/nFgPX6+hpN08RsNov1eh1d18VisYimaWI+nwtZTyxfN/MQGQfbVwhY3MI655KtHqfhaihgHQ6HmM1m0bbtH0FqtVpF0zSx2Wym3GUSydXNPEy2wfZVAha3qGGd+55fLBaP3pVvy1SPt7e3mM1mxzNRQwFrt9sdz1ydOxwO1dSF2+TpZh4q02C7hYDFLWpY5z4E1HAgz1SP/rLfdruNrus+vAfro+9RQ124TZ5u5qEyDbZbCFjcooZ17u/3qeFAnqkey+Uy9vt9RMRNAau/N2vo7Fap+jnb/2xd1336tR99Ta9t21itVmPu6qcWi8VV+/YdebqZh8o02G4hYHGL0tZ5u93GfD6PzWZzPPj3Aas/WOz3+1iv19G2bex2u0fu7pdlrcdXA9bhcIi2baNpmmOdanC6Bm3bRtu2F7+2vwftGo8IWBFx01nJL33/u31nipJ1sD0r9ZhGaeu8Xq9/+4fEcrk83h+0WCyOYavfSrvBOms9vhKwTm+ML239P9J/OvL0vz9ak6Zprg5NjwpY5z/T2HJ2M5PLOtielXpMo7R17s+KXLuVdtmwaR7zDs739/cP9+vagHUarh4RGO6pvzTY2+/3F3/O/hJiv1792axLfXkasPrfe3rmr/+zTv/8fp9Onz02tM+n23n9+u97r7NYZU0X7qa0A03t1GMapa3z4XA4vrmg67o/Atd8Po+u66Lruthut8U9g6lppnkH53/+85/BQHrJNQFrt9sdw1VN911F/D/0nLt0GXCxWBwDz2q1+iP8nAezWwLW+fc4/3POz4r1NRza13uF4bKmC3dT2oGmduoxjZLXuX8G03nAKi1UnZqqHuf3bH43YPWPaxg6y1KDoZAU8ftN773TMHTpzNNpAIv4esC6FJZOv+ZSLc7vibv0s42h3OnCqEo+0NRIPaZR6jqfXoqazWbx9vZWRcjKWo+PAlbt4Soijvf3DWnbdvDerEtfO3RJ76sB69JZp9Nfv/bTtR/t73fl7OYHOb+BtOR7Gb4q62B7VuoxjVLX+fRm9v7Tg6f3uZR6/0/WenwUsPqgW2u4ivg4YPVr0wei80tzp33Zf4/vnsH66F7EobD30d8LAWsip4n3fKvtmvq5rIPtWanHNEpc59MzJqcHqdNLhrPZrMjHA2Stx6WAdfoewqHjRi3Hjo8CVsT/L8f1lwz7dTr/79Pv992A9dV/RPQ1PH/2lYA1kf4vyTPKOtielXpMo9R1PhwOsVgs/njO1dvbWyyXS5cIR3YpYJ1/Oq7Wqx/9ByouWa1Wx8tzpz/vUFiK+PMZWp8FrPN7vS7dN/VZ8Br6/8/3eUw5u/kB+vdGlXpq/buyDrZnpR7TsM65qEdOl4JS7/RDA0M3vJ+eNepD6aWANfR7+kuC/fceekTE6aXKoe8xdEN+/73v9UR33fyvfvFrejDcVxhsuajHNKxzLuqR12f3mfW32Jzrw9npfVDnz8k6P7N0/nsuhaOPPgXah6zT7fz3ew7WRE6vz57eQLpcLot73cQtDLZc1GMa1jkX9cjr3k89fwRPcp/I+Sdwuq6L5XJ5vIGx9pBlsOWiHtOwzrmoR273PNvzCPf+eXTzv1ar1WCQ6k9V3utBZFkYbLmoxzSscy7qkdulJ7qX6J5PcO/VsVJ31j/npOazWLX8pamFekzDOueiHtREN1/hK29Sv+T9/f0hLzG9djPYclGPaVjnXNSDmujmE5c+gto/4f07Aevl5WWSl5jeuhlsuajHNKxzLupBTXRz/P8ZWPP5fPD/7y8RlvrwvmsYbLmoxzSscy7qQU1087/6RzOcPydjs9k8xQNIDbZc1GMa1jkX9aAmuvlfp+/36h/T0Ieukt9Ofy2DLRf1mIZ1zkU9qIluPrHf74+Pa+iff7VaraoPVxEGWzbqMWzsD4tY51zUg5roZiLCYMtGPYb99ddfPtxRMfWgJrqZiDDYslGPYWOvi3XORT2oiW4mIgy2bNRjmIBVN/WgJrqZiDDYslGPYQJW3dSDmuhmIsJgy0Y9hglYdVMPaqKbiQiDLRv1GCZg1U09qIluJiIMtmzUY5iAVTf1oCa6mYgw2LJRj2ECVt3Ug5roZiLCYMtGPYYJWHVTD2qim4kIgy0b9RgmYNVNPaiJbiYiDLZs1GOYgFU39aAmupmIMNiyUY9hAlbd1IOa6GYiwmDLRj2GCVh1Uw9qopuJCIMtG/UYJmDVTT2oiW4mIgy2bNRjmIBVN/WgJrqZiDDYslGPYQJW3dSDmuhmIsJgy0Y9hglYdVMPaqKbiQiDLRv1GCZg1U09qIluJiIMtmzUY5iAVTf1oCa6mYj432D7559/bEk2B5phAlbd1IOa6GYiIuLHjx+2ZBt/ErDqph7URDcDxRCw6qYe1EQ3A8UQsOqmHtRENwPFELDqph7URDcDxRCw6qYe1EQ3A8UQsOqmHtRENwPFELDqph7URDcDxRCw6qYe1EQ3A8UQsOqmHtRENwPFELDqph7URDcDxRCw6qYe1EQ3A8W4R8B69Hsnbd7BSZ10M1CMsQ/Aj37fpM07OKmXgAUUwxkOoBSmFVAMAQsohWl1pbe3t2iaJrque/SuwNMSsIBSmFZXOBwO0batgAUPJmABpTCtrrBer6NpGgELHkzAAkphWn1iu91G0zSxXC4FLHgwAQsohWn1gf7S4GKxOAYtAQseR8ACSmFafaC/NLjb7QQsSEDAAkphWl1wHqgELHg8AQsohWl1wXw+j/l8fvxvAQseT8ACSmFaDei67nhpsCdgweMJWEApnnZaLRaL46MXTh/BsNvtommaWK/Xv339dwPW+/t7/Pr1y2azfWMTsIBSPO20uhSw+rNXn21fDVovLy/x8+dPm832je3vv/++00QAGNfTBqxLttvtMWidbqvVKpqmicViEV3XxXa7ffSuAgBJCVhXcg8WAHAtAetKAhYAcC0B60oCFgBwLQELAGBkAhYAwMgELACAkQlYAAAjE7AAAEYmYAEAjEzAAgAYmYAFADAyAQsAYGQCFgDAyAQsAICRCVgAACMTsAAARiZgAQCMTMACABiZgAUAMDIBCwBgZAIWAMDIBCwAgJEJWAAAIxOwAABGJmABAIxMwAIAGJmABQAwMgELAGBkAhYAwMgELACAkQlYAAAjE7AAAEYmYAEAjEzAAgAYmYAFADAyAQsAYGQCFgDAyAQsAICRCVgAACMTsAAARiZgAQCMTMACABiZgAUAMDIBCwBgZAIWAMDIBCwAgJEJWAAAIxOwAABGJmABAIxMwAIAGJmABQAwMgELAGBkAhYAwMgELACAkQlYAAAjE7AAAEYmYAEAjEzAAgAYmYAFADAyAQsAYGQCFgDAyAQsAICRCVgAACMTsAAARiZgAQCMTMACABiZgAUAMDIBCwBgZAIWAMDIBCwAgJEJWAAAIxOwAABGJmARERG/fv2yJdsAKJeARURENE0TP378sCXZmsZfTYCSmeJERDigJ6MeAGUzxYkIB/Rs1AOgbKY4EeGAno16AJTNFCciHNCzUQ+AspniRIQDejbqAVA2U5yIcEDPRj0AymaKExEO6NmoB0DZTHEiwgE9G/UAKJspTkQ4oGejHgBlM8WJCAf0bNQDoGymOBHhgJ6Nekzj0e+btHkHJ/UyxYkIB/Rs1GMa3sGZa9P31EQ3ExEO6NmoxzSscy7qQU10MxFhsGWjHtOwzrmoBzXRzVd6e3uLpmmi67pH78pdGGy5qMc0rHMu6kFNdPMVDodDtG0rYDEZ9ZiGdc5FPaiJbr7Cer2OpmkELCajHtOwzrmoBzXRzZ/YbrfRNE0sl0sBi8moxzSscy7qQU108wf6S4OLxeIYtAQspqAe07DOuagHNdHNH+gvDe52OwGLSanHNKxzLupBTXTzBeeBSsBiSuoxDeuci3pQE918wXw+j/l8fvxvAYspqcc0rHMu6kFNdPOAruuOlwZ7AhZTUo9pWOdc1IOaPG03LxaL46MXTh/BsNvtommaWK/Xv339dwPW+/v7w1+i+tFmsOWiHtOwzrmoBzV52m6+FLD6s1efbV8NWi8vL/Hz58+0m8GWS2n1KPVNB6Wtc+3Ug5ro5jPb7fYYtE631WoVTdPEYrGIrutiu90+eldHZbDlUlI9Sn7TQUnr/AzUg5ro5iu5B4splVSPkt90UNI6PwP1oCa6+UoCFlMqpR6lv+mglHV+FupBTXTzlQQsplRCPWp400EJ6/xM1IOa6GYiwmDLpoR61PCmgxLW+ZmoBzXRzUSEwZZN9nrU8qaD7Ov8bNSDmuhmIsJgyyZ7PWp500H2dX426kFNdDMRYbBlk7keNb3pIPM6PyP1oCa6mYgw2LJ5dD2mftPBozx6nfmdelAT3UxEGGzZPLoeU7/p4FEevc78Tj2oiW4mIgy2bLLWo7Y3HWRd52elHtRENxMRBls2pdXDJULGoB7URDcTEQZbNqXVQ8BiDOpBTXQzEWGwZVNaPQQsxqAe1EQ3ExEGWzbqMQ3rnIt6UBPdTESUP9j6Myj9VroafoYSWOdc1IOa6GYiovzBJmBxC+uci3pQE91MRJQ/2AQsbmGdc1EPaqKbiYjyB5uAxS2scy7qQU10MxFR/mATsLiFdc5FPaiJbiYiyh9sAha3sM65qAc10c1ERPmDTcDiFtY5F/WgJrqZiCh/sAlY3MI656Ie1EQ3ExFlDLbTAPXdLbumaeLXr1+2O28l9MIzUQ9qopuJiDIG27MFrJ8/f9ruvJXQC89EPaiJbiYiyhhszxawuD/rnIt6UBPdTESUMdi22+3FbbPZ/BagPvra7Xb76B/lUyXUowbWORf1oCa6mYgof7C5yZ1bWOdcstfjcDhE27bRdd2jd4UC5O5mJpN9sH1GwOIW1jmX7PVYrVbRNI2AxVVydzOTyT7YPiNgcQvrnEvmevThSsDiWnm7mUllHmzXELC4RQ3r3Pf8YrF49K58W8Z67Ha7mM/n0TTN8X8FLK6Rr5t5iIyD7SsELG5RwzrPZjMB644Wi0U0TRObzeY4ZwQsrpGvm3mIjIPtKwQsblHDOvcBQMC6j/V6ffzkca0Bq//k9an9fv/H423atr3rfvR/5uvra0REvL6+RtM0sd/vR/1zttvtl2q4WCxuqnm+buYhMg62rxCwuEVp67zdbmM+n8dmszkedPqA1R8A9vt9rNfraNs2drvdI3f3y7LXo8aANRQe+vvNzkNX32tjB57eecC6l1tqOLQen/6eL3011co+2J6NekyjtHVer9e//UNiuVwe7wtaLBbHA2C/bTabR+/yl2SvR20Bq/95Tn121qjvs3vIHLC6rvvyz527m5lM9sH2bNRjGqWtc9u2X3pjQWmXDZvmMe/gfH9/v2r/agtYi8UiVqvVb7/Wtu0fv3bq9fX1t59/tVrFarU69mb/e4cuMX52GbLruk8vEfZf02+nYaz/fudXNE7399Ilz2v39StnscqaLtxNaQea2qnHNEpb58PhcHxzQdd1fwSu+XweXddF13Wx3W7jcDg8epe/pGke8w7Ol5eXq/avpoA1FBhuOYPUX04cCjqnv9YHoz4sDX3NeWg6D1iffY/TwNZ/Tf89Tn/O8xqeh8r+zzk3FEg/UtZ04W5KO9DUTj2mUfI6Hw6H4ycITwNWaaHqVPZ61BSwhs4O9T/fV87S9AHr/NfOb4jvv3cfhlar1R9nWPt9uhSwhta+/4dGxP8D1vnXnD99f+is1lCoPL9MOvRzfSR3NzOZ7IPt2ajHNEpd58PhcLz3ajabxdvbWxUhK3s9agpYQ2dpLgWs04esnl9C+yx09CHp/OzU0CuHPvoUYb9v56Hn9NcvnYH7LGBd+0ncS2e2LsndzYn0A6yGv1hDsg+2Z6Me0yh1nU9vZu9n0ulB8CuXMTLJXo/aA9Y1lwjPQ9hQwDq/t2q73f5xBuurAes8qA0Fvo8C1unfiUtnwk6/39DfIQHrDvoXfNbyF2tI9sH2bNRjGiWu8263O14aPD2wnV4ynM1md/so/T1lr0ftASvi85vcrwlYQ580HApY53/ONQHro77+TsA61a/NpRB2rdzdnMTpR6Nr+Is1JPtgezbqMY1S1/lwOMRisfjjOVdvb2+xXC5dIryTmgLWpUtunwWZawLWUHg6v79q6Pd9dA/WpfB0zdec78/Q2bNzQz/D0H1jH8ndzQn0zbRcLqv5izUk+2B7NuoxDeuci3pM56PLgeePS+j1l6FPg9FQUDq/8f30kuH5J/6GHqFw6Sb3/vuehr/Ty3m3BKyh/Tg/2zb0+66hmz/QXxpcLBZV/ctliMGWi3pMwzrnoh7T+uyxA0P3Op27dJP7+SNE9vv98XjaO79X67NPEUb8ea/U6TH52oB1+j2G9mPoe3gO1sj6S4O73U7AYlLqMQ3rnIt6TGvoSe4M8yT3EZ0HKgGLKanHNKxzLuoxvVtfZPxsvnr2KkLAumg+n8d8Pj/+t4DFlNRjGtY5F/WY3i2Xvp7NV5/g3tPNA/rrs6ef0BGwmJJ6TMM656Ie1ORpu/n8rfN9eNrtdtE0TazX69++/rsB6/39/SEvMb12M9hyUY9pWOdc1IOaPG03XwpY559QuLR9NWi9vLw85CWm124GWy7qMQ3rnIt6UBPdfGa73R6D1unWP3+jvyGwtuvVBlsu6jEN65yLelAT3Xwl92AxJfWYhnXORT2oiW6+koDFlNRjGtY5F/WgJrr5SgIWU1KPYWN/WMQ656Ie1EQ3ExEGWzbqMeyvv/7y4Y6KqQc10c1EhMGWjXoMG3tdrHMu6kFNdDMRYbBlox7DBKy6qQc10c1EhMGWjXoME7Dqph7URDcTEQZbNuoxTMCqm3pQE91MRBhs2ajHMAGrbupBTXQzEWGwZaMewwSsuqkHNdHNRITBlo16DBOw6qYe1EQ3ExEGWzbqMUzAqpt6UBPdTEQYbNmoxzABq27qQU10MxFhsGWjHsMErLqpBzXRzUSEwZaNegwTsOqmHtRENxMRBls26jFMwKqbelAT3UxEGGzZqMcwAatu6kFNdDMRYbBlox7DBKy6qQc10c1EhMGWjXoME7Dqph7URDcTEQZbNuoxTMCqm3pQE91MRBhs2ajHMAGrbupBTXQzEWGwZaMewwSsuqkHNdHNRMT/Bts///xjS7I50AwTsOqmHtRENxMRET9+/LAl2/iTgFU39aAmuhkohoBVN/WgJroZKIaAVTf1oCa6GSiGgFU39aAmuhkohoBVN/WgJroZKIaAVTf1oCa6GSiGgFU39aAmuhkohoBVN/WgJroZKIaAVTf1oCa6GSiGgFU39aAmuhkohoBVN/WgJroZKMY9Ataj3ztp8w5O6qSbgWKMfQB+9Psmbd7BSb0ELKAYznAApTCtgGIIWEApTKsT+/0+VqtVzGazaJom2raNrusevVvAvwQsoBSm1b/2+/0xWK1Wq+i6LhaLRTRNE4vF4tG7B4SABZTDtPrXarWKpmni9fX1t19fLpfRNE28vb09aM+AnoAFlMK0+tdyuYy2bf/49e12G03TuFQICQhYQClMq09sNptomiY2m82jdwWenoAFlMK0uuBwOMTr62vMZrNo2zYOh8OjdwmenoAFlMK0GtB1XTRNc/wk4X6/f/QuASFgAeUwrQZsNpvouu544/tsNovdbvfo3YKnJ2ABpTCtPvH6+no8kwU8loAFlMK0usJ8Po+mab51Fuv9/T1+/fpls9m+sQlYQClMqyv0Dxzdbrc3f4+Xl5f4+fOnzWb7xvb333+P+Dcb4H4ErPjfJwbn8/nFJ7a3bRtN0/gkIQBwFQHrX32IOj9L1T8Ha7lcPmjPAIDSCFj/2u12F99FOJ/Pnb0CAK4mYJ3Y7XaxXC6PQatt21itVsIVAPAlAhYAwMgELACAkQlYAAAjE7AAAEYmYAEAjOy//HNOYCCbW3QAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical gradient check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical gradient check outputs:\n",
      "Derivative of x = -3.9999999999906777\n",
      "Derivative of y = -3.9999999999906777\n",
      "Derivative of z = 3.000000000010772\n",
      "Check if numerical gradient is equal to analytical gradient computed by backpropagation:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# initial conditions\n",
    "x, y, z = -2, 5, -4\n",
    "\n",
    "#numerical gradient check\n",
    "h = 0.0001\n",
    "\n",
    "x_derivative = (forwardCircuit(x+h,y,z) - forwardCircuit(x,y,z)) / h\n",
    "\n",
    "y_derivative = (forwardCircuit(x,y+h,z) - forwardCircuit(x,y,z)) / h\n",
    "\n",
    "z_derivative = (forwardCircuit(x,y,z + h) - forwardCircuit(x,y,z)) / h\n",
    "\n",
    "num_gradient = [x_derivative,y_derivative,z_derivative]\n",
    "\n",
    "print('Numerical gradient check outputs:\\nDerivative of x = {}\\nDerivative of y = {}\\nDerivative of z = {}'\n",
    "      .format(num_gradient[0],num_gradient[1],num_gradient[2]))\n",
    "\n",
    "\n",
    "#check if it is same as analytical gradient computed with backpropagation\n",
    "check = gradient_wtr_xyz == [round(gradient) for gradient in num_gradient]\n",
    "print('Check if numerical gradient is equal to analytical gradient computed by backpropagation:\\n{}'.format(check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example ; Single Neuron**\n",
    "\n",
    "Consider 2-dimensional neuron that computes following function:\n",
    "\n",
    "\\begin{align}\n",
    "f(x,y,a,b,c) = \\sigma(ax + by + c )\n",
    "\\end{align}\n",
    "\n",
    "In this expression, $\\sigma$ is sigmoid function that squashes its input between 0 and 1. Very negative values are squashed towards 0 and positive values towards 1.\n",
    "\n",
    "Sigmoid function is defined as :\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "and its gradient is:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma(x) (1 - \\sigma(x))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigmoid_f = lambda x : 1/(1+np.exp(-x))\n",
    "sigmoid_gradient = lambda x : sigmoid_f(x)*(1-sigmoid_f(x))\n",
    "\n",
    "x = np.linspace(-10,10,500)\n",
    "fig, ax = plt.subplots()\n",
    "#ax.set_xticks(, minor=False)\n",
    "ax.set_yticks(np.arange(0,1.1,step = 0.1))\n",
    "ax.set_xticks(range(-10,11), minor=False)\n",
    "plt.plot(x,sigmoid_f(x),'b',label = 'Sigmoid')\n",
    "plt.plot(x,sigmoid_gradient(x),'g',label = 'Derivative of sigmoid')\n",
    "plt.legend(loc=\"best\")\n",
    "x_middle = len(ax.get_xticklabels())//2\n",
    "ax.get_xticklabels()[x_middle].set_color(\"red\")\n",
    "ax.get_yticklabels()[5].set_color(\"red\")\n",
    "ax.get_ygridlines()[5].set_linewidth(1)\n",
    "ax.get_ygridlines()[5].set_color('black')\n",
    "ax.get_ygridlines()[5].set_linestyle('dashed')\n",
    "ax.get_xgridlines()[x_middle].set_linewidth(1)\n",
    "ax.get_xgridlines()[x_middle].set_color('black')\n",
    "ax.get_xgridlines()[x_middle].set_linestyle('dashed')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"sigmoid\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every unit corresponds to a wire in the diagrams (connections between neurons)\n",
    "class Unit(object):\n",
    "    \"\"\"Stores value carried during forward pass and gradient that flows back through it in backward pass\"\"\"\n",
    "    def __init__(self,value,grad):\n",
    "        # value computed in the forward pass\n",
    "        self.value = value\n",
    "        # derivative of circuit output with respect to (w.r.t.) this unit, computed in backward pass\n",
    "        self.grad = grad\n",
    "    def __str__(self):\n",
    "            return 'Value ={} Gradient = {}'.format(self.value,self.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiplyGate(object):\n",
    "        def forward(self,u0,u1):\n",
    "            'Store pointers to input Units u0 and u1 and outputs unit utop'\n",
    "            self.u0 = u0\n",
    "            self.u1 = u1\n",
    "            self.utop = Unit(u0.value * u1.value,0.0)\n",
    "            return self.utop\n",
    "    \n",
    "        def backward(self):\n",
    "            \"\"\"Takes the gradient in output unit and chain it with the local gradients,\n",
    "            which we derived from multiplyGate before\"\"\"\n",
    "            self.u0.grad += self.u1.value * self.utop.grad\n",
    "            self.u1.grad += self.u0.value * self.utop.grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "class addGate(object):\n",
    "    def forward(self,u0,u1):\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.utop = Unit(u0.value + u1.value,0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Add gate derivative w.r.t to its inputs (derivative = 1)\"\"\"\n",
    "        self.u0.grad += 1 * self.utop.grad\n",
    "        self.u1.grad += 1 * self.utop.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoidGate(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sigmoid = lambda x : (1 / (1 + np.exp(-x)))\n",
    "        \n",
    "    def forward(self,u0):\n",
    "        self.u0 = u0\n",
    "        self.utop = Unit(self.sigmoid(self.u0.value), 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        sig = self.sigmoid(self.u0.value)\n",
    "        self.u0.grad += (sig * (1 - sig)) * self.utop.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit output:Value =0.8807970779778823 Gradient = 0.0\n"
     ]
    }
   ],
   "source": [
    "#create input units\n",
    "a = Unit(1.0,0.0)\n",
    "b = Unit(2.0,0.0)\n",
    "c = Unit(-3.0,0.0)\n",
    "x = Unit(-1.0,0.0)\n",
    "y = Unit(3.0,0.0)\n",
    "\n",
    "#create gates\n",
    "mulg0 = multiplyGate()\n",
    "mulg1 = multiplyGate()\n",
    "addg0 = addGate()\n",
    "addg1 = addGate()\n",
    "sg0 = sigmoidGate()\n",
    "\n",
    "\n",
    "\n",
    "#forward pass\n",
    "\n",
    "def forwardNeuron():\n",
    "    global s,ax,by,axpby,axpbypc\n",
    "    ax = mulg0.forward(a,x) # a*x = -1\n",
    "    by = mulg1.forward(b,y) # b*y = 6\n",
    "    axpby = addg0.forward(ax,by) # a*x + b*y =5\n",
    "    axpbypc = addg1.forward(axpby,c) # a*x + b*y + c = 2\n",
    "    s = sg0.forward(axpbypc) # sigmoid(a*x + b*y + c) = 0.88\n",
    "\n",
    "forwardNeuron()    \n",
    "print('Circuit output:{}'.format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value =1.0 Gradient = -0.10499358540350662\n"
     ]
    }
   ],
   "source": [
    "#Compute gradient\n",
    "s.grad = 1.0 # Sets gradient of the last unit to be 1.0, \n",
    "             #if it was not all gradients would be computed as zero due to multiplication in chain rule\n",
    "\n",
    "sg0.backward()\n",
    "addg1.backward()\n",
    "addg0.backward()\n",
    "mulg1.backward()\n",
    "mulg0.backward()\n",
    "\n",
    "print(a)\n",
    "gradients_backprop = [a.grad,b.grad,c.grad,x.grad,y.grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit output:Value =0.8825501816218984 Gradient = 0.0\n"
     ]
    }
   ],
   "source": [
    "#Change inputs in response to the computed gradients\n",
    "\n",
    "step_size = 0.01\n",
    "a.value += step_size * a.grad\n",
    "b.value += step_size * b.grad\n",
    "c.value += step_size * c.grad\n",
    "x.value += step_size * x.grad\n",
    "y.value += step_size * y.grad\n",
    "\n",
    "forwardNeuron()\n",
    "print('Circuit output:{}'.format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if numerical gradient is equal to analytical gradient computed by backpropagation:\n",
      "True\n",
      "[-0.10499358540350662, 0.31498075621051985, 0.10499358540350662, 0.10499358540350662, 0.20998717080701323]\n",
      "[-0.10499758359205913, 0.3149447748351797, 0.10498958734506125, 0.10498958734506125, 0.2099711788272618]\n"
     ]
    }
   ],
   "source": [
    "#Check correctnes of backpropagation by checking numerical gradient\n",
    "\n",
    "def forwardCircuitFast(a,b,c,x,y):\n",
    "    return 1 / (1 + np.exp( - (a*x + b*y +c)))\n",
    "\n",
    "a, b, c, x, y = 1, 2, -3, -1, 3\n",
    "h = 0.0001\n",
    "\n",
    "a_grad = (forwardCircuitFast(a + h,b,c,x,y) - forwardCircuitFast(a,b,c,x,y)) / h\n",
    "b_grad = (forwardCircuitFast(a,b + h,c,x,y) - forwardCircuitFast(a,b,c,x,y)) / h\n",
    "c_grad = (forwardCircuitFast(a,b,c + h,x,y) - forwardCircuitFast(a,b,c,x,y)) / h\n",
    "x_grad = (forwardCircuitFast(a,b,c,x + h,y) - forwardCircuitFast(a,b,c,x,y)) / h\n",
    "y_grad = (forwardCircuitFast(a,b,c,x,y + h) - forwardCircuitFast(a,b,c,x,y)) / h\n",
    "\n",
    "num_gradient_2 = [a_grad,b_grad,c_grad,x_grad,y_grad]\n",
    "\n",
    "#check if it is same as analytical gradient computed with backpropagation\n",
    "check = [round(gradient) for gradient in gradients_backprop] == [round(gradient) for gradient in num_gradient_2]\n",
    "print('Check if numerical gradient is equal to analytical gradient computed by backpropagation:\\n{}'.format(check))\n",
    "print(gradients_backprop)\n",
    "print(num_gradient_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 1.0\n",
    "\n",
    "x = a * b\n",
    "da = b * dx\n",
    "db = a * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a + b\n",
    "da = 1.0 * dx\n",
    "db = 1.0 * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets compute  x = (a + b + c) in two steps\n",
    "q = a + b #gate 1\n",
    "x = q + c #gate 2\n",
    "\n",
    "dc = 1.0 * dx #backprop gate 2\n",
    "dq = 1.0 * dx\n",
    "\n",
    "da = 1.0 * dq #backprop gate 1\n",
    "db = 1.0 * dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a + b + c\n",
    "\n",
    "da = 1.0 * dx\n",
    "db = 1.0 * dx\n",
    "dc = 1.0 * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a * b + c\n",
    "\n",
    "da = b * dx\n",
    "db = a * dx\n",
    "dc = 1.0 * dx\n",
    "\n",
    "q = a * b\n",
    "\n",
    "x = q + c\n",
    "\n",
    "dq = da * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron in two steps\n",
    "sigmoid = lambda x : (1 / (1 + np.exp(-x)))\n",
    "\n",
    "q = a * x + b*y + c\n",
    "f = sigmoid(q)\n",
    "\n",
    "#now backward pass, we are given df and:\n",
    "df = 1\n",
    "dq = (f* (1-f))*df\n",
    "#now chain it to its inputs\n",
    "da = x * dq\n",
    "dx = a * dq\n",
    "db = y * dq\n",
    "dy = b * dq\n",
    "dc = 1 * dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a * a # value a is flowing into multiplyGate, but it gets split and it both become inputs and backward flow \n",
    "          # of gradients always adds up\n",
    "\n",
    "da  = a * dx # gradient from first branch\n",
    "da += a * dx # now add gradient from second branch\n",
    "\n",
    "#short form\n",
    "\n",
    "da = 2 * a * dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(a) = a^2$\n",
    "\n",
    "its derivative (power rule) : \n",
    "$\\frac{\\partial f(a)}{\\partial a} = 2a$ \n",
    "\n",
    "which is exactly same as being two inputs to a gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a*a + b*b + c*c\n",
    "\n",
    "da = 2*a * dx\n",
    "db = 2*b * dx\n",
    "dc = 2*c * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "\n",
    "x = np.power(((a * b + c) * d),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split expression into manageable chunks of simpler expressions and then chain them with chain rule\n",
    "x1 = a * b + c\n",
    "x2 = x1 * d\n",
    "x = x2 * x2\n",
    "\n",
    "#now we go backwards in backprop\n",
    "\n",
    "dx2 = 2 * x2 * dx #backprop into x2\n",
    "dd = x1 * dx2     #backprop into d\n",
    "dx1 = d * dx2    #backprop into x1\n",
    "da = b * dx1\n",
    "db = a * dx1\n",
    "dc = 1.0 * dx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.0/a #division\n",
    "da = -1.0 /(a*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (a + b)/(c + d)\n",
    "#decompose into simpler expressions\n",
    "x1 = a + b\n",
    "x2 = c + d\n",
    "x3 = 1.0 / x2 \n",
    "x = x1 * x3 # equivalent to above\n",
    "#backpropagation (dont forget reverse order)\n",
    "\n",
    "dx1 = x3 * dx\n",
    "dx3 = x1 * dx\n",
    "dx2 = (-1.0 / (x2*x2)) * dx3 #local gradient as shown above, and chain rule\n",
    "da = 1.0 * dx1\n",
    "db = 1.0 * dx1\n",
    "dc = 1.0 * dx2\n",
    "dd = 1.0 * dx2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process:**\n",
    "\n",
    "- Break down expressions\n",
    "- do the forward pass\n",
    "- for every variable derive its gradient as we go backwards one by one ,apply local gradient and chain them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lambda a,b : np.maximum(a,b) # maximum function passes on the value of the largest input and ignores the other ones\n",
    "                    # gate acts as simple switch, based on which input had the highest value during forward pass\n",
    "                    # other inputs will have zero gradient\n",
    "\n",
    "da = lambda x : 1.0 * dx if a == x else 0.0\n",
    "db = lambda x : 1.0 * dx if b == x else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReLu - Rectified Linear unit\n",
    "x = np.maximum(a,0)\n",
    "da = lambda x : 1.0 * dx if a > 0 else 0.0\n",
    "\n",
    "#gate passes value through if its larger than 0 , or stops the flow and sets it to zero \n",
    "#In the backward pass the gate will pass on the gradient from top if it was activated during forward pass,\n",
    "# or if the original input was below zero , it will stop the gradient flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- We can feed some input through arbitrarily complex real-valued circuit \n",
    "- Tug at the end of circuit with some force\n",
    "- Backpropagation will distribute that tug through entire circuit all the way back to inputs\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Classification**\n",
    "\n",
    "\n",
    "Simple linear classifier:\n",
    "\n",
    "$f(x, y) = ax + by + c$\n",
    "\n",
    "$c =$ bias term\n",
    "\n",
    "$x$ and $y$ are inputs(2-D vector) and $a,b,c$ are parameters of function that we want to learn.\n",
    "\n",
    "Process of Stochaistic Gradient Descent on Neural Networks:\n",
    "\n",
    "- 1. Select random datapoint and feed it through the circuit\n",
    "- 2. We will interpret the output of the circuit as a confidence that the datapoint has class +1 \n",
    "    - i.e. very high values = circuit is very certain datapoint has class +1, and very low values = circuit is certain that datapoint has class -1\n",
    "\n",
    "- 3. We will measure how well the prediction alligns with the provided labels.\n",
    "        - Intuitively, for example, if a positive example scores very low, we will want to tug in into positive direction on the circuit, demanding that it should output higher value for this datapoint.\n",
    "- 4. The circuit will take the tug and backpropagate it to compute tugs on the inputs $a, b, c, x, y$\n",
    "- 5. Since we thinkg of $x,y$ as fixed datapoints, we will ignore the pull on $x,y$\n",
    "- 6. On the other hand , we will take the parameters a,b,c and make them respond to their tug (we will perform parameter update). This will make it so that circuit will output slightly higher score on this particular datapoint in the future\n",
    "- 7. Iterate! Go back to step 1.\n",
    "        \n",
    "**As far as circuit is concerned, $a,b,c,x,y$ are all made up of the same stuff**\n",
    "- However, after the backward pass is complete , we ignore all changes on datapoints $x,y$ and keep swapping them in and out as we iterate over examples in the dataset\n",
    "- On the other hand, we keep parameters $a,b,c$ around and keep tugging on them every time we sample a datapoint.\n",
    "- Over time, the pulls on these parameters will tune these values in such way, that the function outputs high scores for positive examples and low scores for negative examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning a Support Vector Machine**\n",
    "\n",
    "Support Vector Machine \"Force Specification\"\n",
    "\n",
    "- If we feed a positive datapoint through the SVM circuit, and the output value is less then $1$, pull on the circuit with force $+1$.This is positive example, so we want the score to be higher for it.\n",
    "- Conversely, if we feed it a negative datapoint through the SVM circuit, and the output value is greater then $-1$, then the circuit is giving then the circuit is giving datapoint dangerously high score >> Pull on the circuit downwards with force $-1$\n",
    "- In addition to the pulls above, always add a small amount of pull on the parameters $a,b$(not on $c$!), that pulls them towards zero. You can think of both $a,b$ as being attached to physical spring that is attached at zero. Just as with physical spring, this will make the pull proportional to the value of each $a,b$. For example if $a$ becomes very high, it will experience a strong pull of magnitude $|a|$ back towards zero. This pull is **Regularization** and it ensures that neither of our parameters gets disproportionally large.\n",
    "    - This would be undesirable because both $a,b$ gets multiplied to the input features $x,y$, so if either of them is too high, our classifier would be overly sensitive to this features. This is not nice property, because features can be often noisy in practice, so we want our classifier to change relatively smoothly if they wiggle around.\n",
    "\n",
    "\n",
    "Concrete example\n",
    "\n",
    "vector   -> label\n",
    "\n",
    "[1.2, 0.7] -> +1\n",
    "\n",
    "[-0.3, 0.5] -> -1\n",
    "\n",
    "[-3, -1] -> +1\n",
    "\n",
    "[0.1, 1.0] -> -1\n",
    "\n",
    "[3.0, 1.1] -> -1\n",
    "\n",
    "[2.1, -3] -> +1\n",
    "\n",
    "random parameter setting: $a = 1 ,  b = -2 ,  c = -1$\n",
    "\n",
    "data point = $[1.2,0.7]$\n",
    "\n",
    "SVM will compute $1 \\cdot 1.2 + (-2) \\cdot 0.7 - 1 = -1.2$\n",
    "- this point is labeled as $+1$ in the training data , so we want the score be higher than $1$.\n",
    "    - The gradient on top of the circuit will thus be positive $+1$, which will backpropagate to $a,b,c$.\n",
    "    - Additionaly, there will also be reguralization pull on $a$ of $-1$ (to make it smaller) and reguralization pull on $b$ of $+2$ to make it larger, toward zero.\n",
    "\n",
    "Supposes that instead we fed another datapoint SVM:\n",
    "\n",
    "data point = $[-0.3,0.5]$ >>> $1 \\cdot -0.3 + (-2) \\cdot 0.5 - 1 = -2.3$\n",
    "- Label of this datapoint is $-1$ and since $-2.3$ is smaller then $-1$,we see that according to our force specification the SVM should be happy: The computed score is very negative, consistent with the negative labe of this example.\n",
    "    - There will be no pull at the end of the circuit(i.e. its zero), since there are no changes necessary\n",
    "    - However, there will still be the reguralization pull on $a$ of $-1$ and on $b$ of $2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A circuit: it takes 5 Units(x,y,a,b,c) and outputs single Unit\n",
    "#It can also compute gradient w.r.t. its inputs\n",
    "\n",
    "class Circuit():\n",
    "    \"\"\"Computes (a*x + b*y + c) and gradient\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Create some gates\"\"\"\n",
    "        self.mulg0 = multiplyGate()\n",
    "        self.mulg1 = multiplyGate()\n",
    "        self.addg0 = addGate()\n",
    "        self.addg1 = addGate()\n",
    "        \n",
    "    def forward(self,x,y,a,b,c):\n",
    "        self.ax = self.mulg0.forward(a,x) #a*x\n",
    "        self.by = self.mulg1.forward(b,y) #b*y\n",
    "        self.axpby = self.addg0.forward(self.ax,self.by) #a*x + b*y\n",
    "        self.axpbypc = self.addg1.forward(self.axpby,c) # (a*x + b*y) + c\n",
    "        return self.axpbypc\n",
    "    \n",
    "    def backward(self,gradient_top):\n",
    "        \"\"\"Takes pull from above\"\"\"\n",
    "        self.axpbypc.grad = gradient_top\n",
    "        self.addg1.backward() # sets gradient in axpby and c\n",
    "        self.addg0.backward() # sets gradient in ax and by\n",
    "        self.mulg1.backward() # sets gradient in b and y\n",
    "        self.mulg0.backward() # sets gradient in a and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM():\n",
    "    \"\"\"SVM class\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"random initial parameters\"\"\"\n",
    "        self.a = Unit(1.0,0.0)\n",
    "        self.b = Unit(-2.0,0.0)\n",
    "        self.c = Unit(-1.0,0.0)\n",
    "        \n",
    "        self.circuit = Circuit()\n",
    "    \n",
    "    def values(self):\n",
    "        return self.a.value,self.b.value,self.c.value\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        \"\"\"Expects x and y to be Units\"\"\"\n",
    "        self.unit_out = self.circuit.forward(x, y, self.a, self.b, self.c)\n",
    "        return self.unit_out\n",
    "    \n",
    "    \n",
    "    def backward(self,label):\n",
    "        \"\"\"label is +1 or -1\"\"\"\n",
    "        \n",
    "        #reset pulls on a,b,c\n",
    "        self.a.grad = 0.0\n",
    "        self.b.grad = 0.0\n",
    "        self.c.grad = 0.0\n",
    "        label = label\n",
    "        \n",
    "        #compute the pull based on what circuit output was\n",
    "        pull = 0.0\n",
    "        \n",
    "        if (label == 1 and self.unit_out.value < 1): \n",
    "            pull = 1  # score was too low: pull up\n",
    "            \n",
    "        elif (label == -1 and self.unit_out.value > -1):\n",
    "            pull = -1  # score was too high for positive example : pull down\n",
    "            \n",
    "        self.circuit.backward(pull) # writes gradient into x,y,a,b,c\n",
    "        \n",
    "        #add regularization pull for parameters: towards zero and proportional to value\n",
    "        self.a.grad += -self.a.value\n",
    "        self.b.grad += -self.b.value\n",
    "\n",
    "    \n",
    "    def learnFrom(self,x,y,label):\n",
    "        self.forward(x,y) # forward pass (set .value in all Units)\n",
    "        self.backward(label) # backward pass (set .grad in all Units)\n",
    "        self.parameterUpdate() # parameters responds to tug\n",
    "    def parameterUpdate(self):\n",
    "        step_size = 0.01\n",
    "        self.a.value += step_size * self.a.grad\n",
    "        self.b.value += step_size * self.b.grad\n",
    "        self.c.value += step_size * self.c.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train SVM with Stochastic Gradient Descent\n",
    "\n",
    "data, labels = [], []\n",
    "data.append([1.2,0.7]);labels.append(1)\n",
    "data.append([-0.3, -0.5]);labels.append(-1)\n",
    "data.append([3.0, 0.1]);labels.append(1)\n",
    "data.append([-0.1, -1.0]);labels.append(-1)\n",
    "data.append([-1.0, 1.1]);labels.append(-1)\n",
    "data.append([2.1, -3]);labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy at iter 0 : 0.6666666666666666\n",
      "training accuracy at iter 25 : 0.6666666666666666\n",
      "training accuracy at iter 50 : 0.8333333333333334\n",
      "training accuracy at iter 75 : 0.8333333333333334\n",
      "training accuracy at iter 100 : 0.8333333333333334\n",
      "training accuracy at iter 125 : 0.8333333333333334\n",
      "training accuracy at iter 150 : 0.8333333333333334\n",
      "training accuracy at iter 175 : 0.8333333333333334\n",
      "training accuracy at iter 200 : 0.8333333333333334\n",
      "training accuracy at iter 225 : 0.8333333333333334\n",
      "training accuracy at iter 250 : 0.8333333333333334\n",
      "training accuracy at iter 275 : 0.8333333333333334\n",
      "training accuracy at iter 300 : 0.8333333333333334\n",
      "training accuracy at iter 325 : 0.8333333333333334\n",
      "training accuracy at iter 350 : 0.8333333333333334\n",
      "training accuracy at iter 375 : 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "svm = SVM()\n",
    "\n",
    "def evalTrainingAccuracy():\n",
    "    \"\"\"function that evaluates training accuracy\"\"\"\n",
    "    \n",
    "    num_correct = 0\n",
    "    \n",
    "    for feature,label in zip(data,labels):\n",
    "        x = Unit(feature[0],0.0)\n",
    "        y = Unit(feature[1],0.0)\n",
    "        true_label = label\n",
    "        #see if prediction matches provided label \n",
    "        predicted_label =  1 if svm.forward(x,y).value > 0 else -1\n",
    "        if predicted_label == true_label:\n",
    "            num_correct += 1\n",
    "    return num_correct / len(data)\n",
    "\n",
    "#learning loop\n",
    "import random\n",
    "import math\n",
    "\n",
    "def learning(n_iter):\n",
    "    for iteration in range(0,n_iter):\n",
    "        #pick random datapoint\n",
    "        i = math.floor(random.random() * len(data))\n",
    "        x = Unit(data[i][0],0.0)\n",
    "        y = Unit(data[i][1],0.0)\n",
    "        label = labels[i]\n",
    "        svm.learnFrom(x,y,label)\n",
    "    \n",
    "        if iteration % 25 == 0: #every 10 iterations\n",
    "            print('training accuracy at iter {} : {}'.format(iteration,evalTrainingAccuracy()))\n",
    "            #print('a = {}, b = {}, c = {}'.format(*svm.values()))\n",
    "            \n",
    "learning(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generalizing the SVM into a Neural Network**\n",
    "\n",
    "\n",
    "2-layer Neural Network for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume inputs x,y\n",
    "\n",
    "# n1 = np.maximum(0,a1*x + b1*y + c1) #activation of first hidden neuron\n",
    "# n2 = np.maximum(0,a2*x + b2*y + c2) #2nd neuron\n",
    "# n3 = np.maximum(0,a3*x + b3*y + c3) #3rd neuron\n",
    "\n",
    "# score = a4*n1 +b4*n2 + c4*n3 +d4 # score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specification above is 2-layer Neural Network with 3 hidden neurons(n1,n2,n3), that uses Rectified Linear Unit(ReLu)\n",
    "            non-linearity on each hidden neuron. As you can see, there are now several parameters involved, which means that our classifier is more complex and can represent more intricate decision boundaries than just a simple linear decision rule such as SVM. Another way to think about it is that every one of three hidden neurons is a linear classifier and now we are putting an extra linear classifier on top of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random initial parameters\n",
    "\n",
    "a1 =  random.random() - 0.5 # random number between -0.5 and 0.5\n",
    "b1 =  random.random() - 0.5\n",
    "c1 =  random.random() - 0.5\n",
    "a2 =  random.random() - 0.5 # random number between -0.5 and 0.5\n",
    "b2 =  random.random() - 0.5\n",
    "c2 =  random.random() - 0.5\n",
    "a3 =  random.random() - 0.5 # random number between -0.5 and 0.5\n",
    "b3 =  random.random() - 0.5\n",
    "c3 =  random.random() - 0.5\n",
    "a4 =  random.random() - 0.5 # random number between -0.5 and 0.5\n",
    "b4 =  random.random() - 0.5\n",
    "c4 =  random.random() - 0.5\n",
    "d4 = random.random() - 0.5\n",
    "\n",
    "for n_iter in range(0,400):\n",
    "    #pick random datapoint\n",
    "    i = math.floor(random.random() * len(data))\n",
    "    x = data[i][0]\n",
    "    y = data[i][0]\n",
    "    label = labels[i]\n",
    "    \n",
    "    \n",
    "    #compute forward pass\n",
    "    n1 = np.maximum(0,a1*x + b1*y + c1) #activation of first hidden neuron\n",
    "    n2 = np.maximum(0,a2*x + b2*y + c2) #2nd neuron\n",
    "    n3 = np.maximum(0,a3*x + b3*y + c3) #3rd neuron\n",
    "    score = a4*n1 +b4*n2 + c4*n3 + d4 # score\n",
    "\n",
    "    #compute the pull on top\n",
    "    pull = 0.0\n",
    "    pull = 1 if (label == 1 and score < 1 ) else pull  #we want higher output, Pull up\n",
    "    pull = 1 if (label == -1 and score > -1 ) else pull #we want lower output, Pull down\n",
    "    \n",
    "    #now compute backward pass to all parameters of model\n",
    "    \n",
    "    #backprop through last score neuron\n",
    "    dscore = pull\n",
    "    da4 = n1 * dscore\n",
    "    db4 = n2 * dscore\n",
    "    dc4 = n3 * dscore\n",
    "    dd4 = 1.0 * dscore\n",
    "    \n",
    "    #backprop to ReLu nonlinearities\n",
    "    #i.e. just set gradients to 0 if the neurons did not \"fire\"\n",
    "    dn3 = 0 if n3 == 0 else dn3\n",
    "    dn2 = 0 if n2 == 0 else dn2\n",
    "    dn1 = 0 if n1 == 0 else dn1\n",
    "    \n",
    "    #backprop to parameters of neuron1\n",
    "    da1 = x * dn1\n",
    "    db1 = y * dn1\n",
    "    dc1 = 1.0 * dn1\n",
    "    \n",
    "    \n",
    "    #backprop to parameters of neuron2\n",
    "    da2 = x * dn2\n",
    "    db2 = y * dn2\n",
    "    dc2 = 1.0 * dn2\n",
    "    \n",
    "    #backprop to parameters of neuron3\n",
    "    da3 = x * dn3\n",
    "    db3 = y * dn3\n",
    "    dc3 = 1.0 * dn3\n",
    "\n",
    "    \n",
    "    #add pulls from reguralization ,tugging all multiplicative parameters downward \n",
    "    #(i.e. not the biases) downward,proportional to their values\n",
    "    da1 += -a1; da2 += -a2; da3 += -a3;\n",
    "    db1 += -b1; db2 += -b2; db3 += -b3;\n",
    "    da4 += -a4; db4 += -b4; dc4 += -c4;\n",
    "    \n",
    "    #do the parameter update\n",
    "    step_size = 0.01\n",
    "    \n",
    "    a1 += da1 * step_size\n",
    "    a2 += da2 * step_size\n",
    "    a3 += da3 * step_size\n",
    "    a4 += da4 * step_size\n",
    "    b1 += db1 * step_size\n",
    "    b2 += db2 * step_size\n",
    "    b3 += db3 * step_size\n",
    "    b4 += db4 * step_size\n",
    "    c1 += dc1 * step_size\n",
    "    c2 += dc2 * step_size\n",
    "    c3 += dc3 * step_size\n",
    "    c4 += dc4 * step_size\n",
    "    d4 += dd4 * step_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-layer Neural Net: \n",
    "\n",
    "- we write forward pass expression\n",
    "- interpret the value at the end as a score\n",
    "- pull on that value in a positive or negative direction depending on what we want that value to be for our \n",
    "particular example\n",
    "- parameter update after backprop will ensure that when we see this particular example in the future,the network will be more likely to give us value we desire , not the one it gave just before update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conventional Approach: Loss Functions**\n",
    "\n",
    "Example: 2-D Support Vector Machine\n",
    "\n",
    "Dataset of $N$ examples $(x_{i0}, x_{i1})$ and their corresponding target features $(y_{i})$, which are allowed to be either $+1 / -1$ for positive or negative examples respectively.\n",
    "\n",
    "We have three weight parameters $(w_0, w_1, w_2)$\n",
    "\n",
    "SVM loss function is defined as follows:\n",
    "\n",
    "$$ L = [\\sum_{i=1}^N max(0, -y_{i}( w_0x_{i0} + w_1x_{i1} + w_2 ) + 1 )] + \\alpha [w_0^2 + w_1^2] $$\n",
    "\n",
    "Expression is always positive, due to thresholding at zero in the first expression and the squaring in the reguralization.\n",
    "The idea is that we will want this expression to be as small as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[1.2,0.7],[-0.3,0.5],[3,2.5]] # array of 2-dimensional data\n",
    "y = [1,-1,1] # array of labels \n",
    "w = [0.1,0.2,0.3] # example: random numbers\n",
    "alpha = 0.1 # reguralization strength\n",
    "\n",
    "def cost(X,y,w):\n",
    "    \"\"\"SVM cost function\"\"\"\n",
    "    \n",
    "    total_cost = 0.0 #L, in SVM loss function expression above\n",
    "    N = len(X)\n",
    "    # loop over all data points and compute their score\n",
    "    for i,feature in enumerate(X):\n",
    "        score = w[0] * feature[0] + w[1] * feature[1] + w[2]\n",
    "        \n",
    "    # accumulate cost based on how compatible the score is with the label\n",
    "        yi = y[i]\n",
    "        costi = np.maximum(0,-yi * score + 1)\n",
    "        print('example {}: xi = {} and label = {}'.format(i,feature,yi))\n",
    "        print('score computed to be: {}'.format(round(score,3)))\n",
    "        print('==> cost computed to be: {}'.format(round(costi,3)))\n",
    "        total_cost += costi\n",
    "    \n",
    "   #reguralization cost: we want small weights\n",
    "\n",
    "    reg_cost = alpha * (w[0] * w[0] + w[1] * w[1])\n",
    "    print('reguralization cost for current model is {}'.format(round(reg_cost,3)))\n",
    "    total_cost += reg_cost\n",
    "    \n",
    "    print('total cost is {}'.format(round(total_cost,3)))\n",
    "    return total_cost   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 0: xi = [1.2, 0.7] and label = 1\n",
      "score computed to be: 0.56\n",
      "==> cost computed to be: 0.44\n",
      "example 1: xi = [-0.3, 0.5] and label = -1\n",
      "score computed to be: 0.37\n",
      "==> cost computed to be: 1.37\n",
      "example 2: xi = [3, 2.5] and label = 1\n",
      "score computed to be: 1.1\n",
      "==> cost computed to be: 0.0\n",
      "reguralization cost for current model is 0.005\n",
      "total cost is 1.815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.815"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(X,y,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cost function is an expression that measures how bad is your classifier. When the training set is perfectly classified, the cost(ignoring the reguralization ) will be zero.*\n",
    "\n",
    "\n",
    "**The majority of cost functions in Machine Learning consists of two parts.**\n",
    "- 1. A part that measures how well model fits the data.\n",
    "- 2. Reguralization, which measures some notion of how complex or likely model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "62px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "495.85px",
    "left": "1581px",
    "right": "20px",
    "top": "121px",
    "width": "326px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
