{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "Outlier Detection in general refers to the task of identifying observations which may be considered anomalous given the distribution of a sample. Any observation belonging to the distribution is reffered to as an inlier and  any outlying point is reffered to as an outlier. \n",
    "\n",
    "In the context of machine learning, there three common approaches for this task.\n",
    "\n",
    "1. **Unsupervised Outlier Detection**\n",
    "    - Training data (unlabelled) contains both normal and anomalous observations.\n",
    "    - The model identifies outliers during the fitting process \n",
    "    - This approach is taken when outliers are defined as points that exists in low-density regions in the data\n",
    "    - Any new observations that do not belong to high-density regions are considered outliers\n",
    "    \n",
    "\n",
    "2. **Semi-supervised Novelty Detection**\n",
    "    - Training data consists only of observations describing normal behavior.\n",
    "    - The model is fit on training data and then used to evaluate new observations.\n",
    "    - This approach is taken when outliers are defined as points differing from the distribution of the training data.\n",
    "\n",
    "\n",
    "3. **Supervised Outlier Classification**\n",
    "    - The ground truth label (inlier vs outlier) for every observation is known.\n",
    "    - The model is fit on imbalanced training data and then used to classify new observations.\n",
    "    - This approach is taken when ground truth is available and it is assumed that outliers will follow the same distribution as in the training set.\n",
    "    - Any new observations are classified using the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \"Definitions\" of Anomalies\n",
    "\n",
    "### Distance-based or density-based definitions:\n",
    "\n",
    "1. D-distance Anomalies are instances which have fewer than p neighbouring points within distance D.\n",
    "\n",
    "2. kth NN Distance Anomalies are the top-ranked instances whose distance to the kth nearest neighbor is greatest\n",
    "\n",
    "3. Average kNN are the top-ranked instances whose average distance to the k nearest neighbors is greatest.\n",
    "\n",
    "4. Density-based Anomalies which are in regions of low density or low local /relative density\n",
    "    \n",
    "### Isolation based are instances which are most susceptible to isolation\n",
    "\n",
    "\n",
    "### ZERO++: anomalies are instances which have the highest number of subspaces having zero appearances (over set of subsamples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection using Gaussian distribution\n",
    "\n",
    "\n",
    "\n",
    "**Problem Motivation**\n",
    "\n",
    "\n",
    "We are given a dataset $x^{(1)}, x^{(2)},\\dots,x^{(m)}$\n",
    "- We are then given a new example, $x_{test}$, and we want to know whether this new example is abnormal/anomalous.\n",
    "- We define a \"model\" $p(x)$ that tells us the probability the example is not anomalous. We also use a threshold $\\epsilon$ (epsilon) as a dividing line so we can say which examples are anomalous and which are not.\n",
    "\n",
    "**p(x) is not a probability but rather the normalized probability density as parameterized by the feature vector, x; therefore, ϵ is a threshold condition on the probability density.**\n",
    "\n",
    "A very common application of anomaly detection is detecting fraud:\n",
    "\n",
    "- $x^{(i)}$ = features of user i's activities\n",
    "- Model $p(x)$ from the data.\n",
    "- Identify unusual users by checking which have $p(x)<ϵ$\n",
    "\n",
    "If our anomaly detector is flagging **too many** anomalous examples, then we need to **decrease** our threshold ϵ\n",
    "\n",
    "\n",
    "## Gaussian Distribution\n",
    "\n",
    "described by a function $\\mathcal{N}(\\mu,\\sigma^2)$\n",
    "\n",
    "- $X \\in ℝ$\n",
    "- If the probability distribution of x is Gaussian with mean $\\mu$, variance $\\sigma^2$, then:\n",
    "    - $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$\n",
    "        - $\\sim$ = 'tilde' = distributed as\n",
    "- The Gaussian Distribution is parameterized by a mean and a variance.\n",
    "    - $\\mu$ describes center of curve, called mean\n",
    "    - $\\sigma$ describs width of the curve , in other words, standard deviation\n",
    "- function is :\n",
    "$\\large p(x;\\mu,\\sigma^2) = \\dfrac{1}{\\sigma\\sqrt{(2\\pi)}}e^{-\\dfrac{1}{2}(\\dfrac{x - \\mu}{\\sigma})^2}$\n",
    "    \n",
    "We can estimate the parameter μ from a given dataset by simply taking the average of all the examples:\n",
    "\n",
    "$\\mu = \\dfrac{1}{m}\\sum_{i=1}^{m}x^{(i)}$\n",
    "\n",
    "\n",
    "We can estimate the other parameter, $\\sigma^2$, with squared error formula:\n",
    "\n",
    "$\\sigma^2 = \\dfrac{1}{m}\\sum_{i=1}^{m}(x^{(i)}- \\mu)^2$\n",
    "\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "Given a training set of examples, {$x^{(1)}\\dots x^{(m)}$} where each example is a vector, $x \\in ℝ^n$.\n",
    "\n",
    "$p(x) = p(x_1;\\mu_1,\\sigma_1^2)p(x_2;\\mu_2,\\sigma^2_2)\\cdots p(x_n;\\mu_n,\\sigma^2_n)$\n",
    "\n",
    "(aka  \"independence assumption\" on the values of the features inside training example x.)\n",
    "\n",
    "$= \\displaystyle \\prod^n_{j=1} p(x_j;\\mu_j,\\sigma_j^2)$\n",
    "\n",
    "\n",
    "**The Algorithm**\n",
    "\n",
    "- Choose features $x_i$ that you think might be indicative of anomalous examples.\n",
    "- Fit parameters $\\mu_1,\\dots,\\mu_n,\\sigma_1^2,\\dots,\\sigma_n^2$\n",
    "- Calculate $\\mu_j = \\dfrac{1}{m}\\sum_{i=1}^{m}x_{j}^{(i)}$\n",
    "- Calculate $\\sigma_{j}^{2} = \\dfrac{1}{m}\\sum_{i=1}^{m}(x_{j}^{(i)}- \\mu_j)^2$\n",
    "- Given new example,compute $p(x)$:\n",
    "$$p(x) = \\displaystyle \\prod^n_{j=1} p(x_j;\\mu_j,\\sigma_j^2) = \\prod\\limits^n_{j=1} \\dfrac{1}{\\sqrt{2\\pi}\\sigma_j}exp(-\\dfrac{(x_j - \\mu_j)^2}{2\\sigma^2_j})$$\n",
    "\n",
    "- Anomaly if p(x)<ϵ\n",
    "\n",
    "A vectorized version of the calculation for μ is $\\mu = \\dfrac{1}{m}\\displaystyle \\sum_{i=1}^m x^{(i)}$ \n",
    "\n",
    "A vectorized version of the calculation for $\\sigma^2$ is $\\sigma^2 = \\dfrac{1}{m}\\sum_{i=1}^{m}(x^{(i)}- \\mu)^2$\n",
    "\n",
    "## Developing and Evaluating an Anomaly Detection System  \n",
    "\n",
    "To evaluate our learning algorithm, we take some labeled data, categorized into anomalous and non-anomalous examples ( y = 0 if normal, y = 1 if anomalous).\n",
    "\n",
    "Among that data, take a large proportion of **good**, non-anomalous data for the training set on which to train p(x).\n",
    "\n",
    "Then, take a smaller proportion of mixed anomalous and non-anomalous examples (you will usually have many more non-anomalous examples) for your cross-validation and test sets.\n",
    "\n",
    "For example, we may have a set where 0.2% of the data is anomalous. We take 60% of those examples, all of which are good (y=0) for the training set. We then take 20% of the examples for the cross-validation set (with 0.1% of the anomalous examples) and another 20% from the test set (with another 0.1% of the anomalous).\n",
    "\n",
    "\n",
    "In other words, we split the data 60/20/20 training/CV/test and then split the anomalous examples 50/50 between the CV and test sets.\n",
    "\n",
    "**Algorithm evaluation:**\n",
    "\n",
    "Fit model p(x) on training set $\\lbrace x^{(1)},\\dots,x^{(m)} \\rbrace$\n",
    "\n",
    "On a cross validation/test example x, predict:\n",
    "\n",
    "If p(x) < ϵ (**anomaly**), then y=1\n",
    "\n",
    "If p(x) ≥ ϵ (**normal**), then y=0\n",
    "\n",
    "Possible evaluation metrics :\n",
    "\n",
    "- True positive, false positive, false negative, true negative.\n",
    "\n",
    "- Precision/recall\n",
    "\n",
    "- $F_1$ score\n",
    "\n",
    "**use the cross-validation set to choose parameter** ϵ\n",
    "\n",
    "\n",
    "## Anomaly Detection vs. Supervised Learning\n",
    "\n",
    "Use anomaly detection when...\n",
    "\n",
    "- We have a very small number of positive examples (y=1 ... 0-20 examples is common) and a large number of negative (y=0) examples.\n",
    "\n",
    "- We have many different \"types\" of anomalies and it is hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we've seen so far.\n",
    "\n",
    "Use supervised learning when...\n",
    "\n",
    "- We have a large number of both positive and negative examples. In other words, the training set is more evenly divided into classes. (no skewed classes)\n",
    "\n",
    "\n",
    "- We have enough positive examples for the algorithm to get a sense of what new positives examples look like. The future positive examples are likely similar to the ones in the training set. \n",
    "\n",
    "\n",
    "## Choosing What Features to Use\n",
    "\n",
    "The features will greatly affect how well your anomaly detection algorithm works.\n",
    "\n",
    "We can check that our features are **gaussian** by plotting a histogram of our data and checking for the bell-shaped curve.\n",
    "\n",
    "Some **transforms** we can try on an example feature x that does not have the bell-shaped curve are:\n",
    "\n",
    "- log(x)\n",
    "- log(x+1)\n",
    "- log(x+c) for some constant\n",
    "- $\\sqrt x$\n",
    "- $x^{1/3}$\n",
    "\n",
    "We can play with each of these to try and achieve the gaussian shape in our data.\n",
    "\n",
    "There is an **error analysis procedure** for anomaly detection that is very similar to the one in supervised learning.\n",
    "\n",
    "- Our goal is for p(x) to be large for normal examples and small for anomalous examples.\n",
    "- One common problem is when p(x) is similar for both types of examples. In this case, you need to examine the anomalous examples that are giving high probability in detail and try to figure out new features that will better distinguish the data.\n",
    "\n",
    "- In general, choose features that might take on unusually large or small values in the event of an anomaly.\n",
    "\n",
    "## Multivariate Gaussian Distribution\n",
    "\n",
    "The multivariate gaussian distribution is an extension of anomaly detection and may (or may not) catch more anomalies.\n",
    "\n",
    "Instead of modeling $p(x_1),p(x_2),\\dots$ separately, we will model p(x) all in one go.\n",
    "\n",
    "- Our parameters will be:\n",
    "    - $\\mu \\in \\mathbb{R}^n$\n",
    "    - $\\Sigma \\in \\mathbb{R}^{n \\times n}$\n",
    "\n",
    "$$p(x;\\mu,\\Sigma) = \\dfrac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}} exp(-1/2(x-\\mu)^T\\Sigma^{-1}(x-\\mu))$$\n",
    "\n",
    "\n",
    "The important effect is that we can model oblong gaussian contours, allowing us to better fit data that might not fit into the normal circular contours. \\\n",
    "- ( oblong = 'a rectangular object or flat figure with unequal adjacent sides')\n",
    "\n",
    "Varying $\\Sigma$ changes the shape, width, and orientation of the contours. Changing $\\mu$ will move the center of the distribution.\n",
    "\n",
    "\n",
    "- When doing anomaly detection with multivariate gaussian distribution, we compute μ and Σ normally. We then compute p(x) using the new formula in the previous section and flag an anomaly if p(x) < ϵ.\n",
    "\n",
    "- The original model for p(x) corresponds to a multivariate Gaussian where the contours of p(x;μ,Σ) are axis-aligned.\n",
    "\n",
    "- The multivariate Gaussian model can automatically capture correlations between different features of x.\n",
    "\n",
    "However, the original model maintains some advantages: it is computationally cheaper (no matrix to invert, which is costly for large number of features) and it performs well even with small training set size (in multivariate Gaussian model, it should be greater than the number of features for Σ to be invertible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariateGaussian(X, mu, Sigma2):\n",
    "    \"\"\"\n",
    "    Computes the probability density function of the multivariate gaussian distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n). Where there are m examples of n-dimensions.\n",
    "\n",
    "    mu : array_like\n",
    "        A vector of shape (n,) contains the means for each dimension (feature).\n",
    "\n",
    "    Sigma2 : array_like\n",
    "        Either a vector of shape (n,) containing the variances of independent features\n",
    "        (i.e. it is the diagonal of the correlation matrix), or the full\n",
    "        correlation matrix of shape (n x n) which can represent dependent features.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    p : array_like\n",
    "        A vector of shape (m,) which contains the computed probabilities at each of the\n",
    "        provided examples.\n",
    "    \"\"\"\n",
    "    k = mu.size\n",
    "\n",
    "    # if sigma is given as a diagonal, compute the matrix\n",
    "    if Sigma2.ndim == 1:\n",
    "        Sigma2 = np.diag(Sigma2)\n",
    "\n",
    "    X = X - mu\n",
    "    p = (2 * np.pi) ** (- k / 2) * np.linalg.det(Sigma2) ** (-0.5)\\\n",
    "        * np.exp(-0.5 * np.sum(np.dot(X, np.linalg.pinv(Sigma2)) * X, axis=1))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
