{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation I: Data Compression**\n",
    "\n",
    "- We may want to reduce the dimension of our features if we have a lot of redundant data.\n",
    "- To do this, we find two highly correlated features, plot them, and make a new line that seems to describe both features accurately. We place all the new features on this single line\n",
    "\n",
    "- Doing dimensionality reduction will reduce the total data we have to store in computer memory and will speed up our learning algorithm.\n",
    "\n",
    "\n",
    "**Motivation II: Visualization**\n",
    "\n",
    "- It is not easy to visualize data that is more than three dimensions. We can reduce the dimensions of our data to 3 or less in order to plot it.\n",
    "\n",
    "- We need to find new features, $z_1, z_2$(and perhaps $z_3$) that can effectively summarize all the other features.\n",
    "\n",
    "Example: hundreds of features related to a country's economic system may all be combined into one feature that you call \"Economic Activity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "## **Problem formulation**\n",
    "\n",
    "Given two features, $x_1$ and $x_2$, we want to find a single line that effectively describes both features at once. We then map our old features onto this new line to get a new single feature.\n",
    "\n",
    "The same can be done with three features, where we map them to a plane.\n",
    "\n",
    "The goal of PCA is to reduce the average of all the distances of every feature to the projection line. This is the projection error.\n",
    "\n",
    "Reduce from 2d to 1d: find a direction (a vector $u(1)\\in R^n$) onto which to project the data so as to minimize the projection error.\n",
    "\n",
    "General case :\n",
    "\n",
    "Reduce from n-dimension to k-dimension: Find $k$ vectors $u^{(1)}, u^{(2)}, \\dots, u^{(k)}$ onto which to project the data so as to minimize the projection error.\n",
    "\n",
    "If we are converting from 3d to 2d, we will project our data onto two directions (a plane), so $k$ will be 2.\n",
    "\n",
    "## Principal Component Analysis Algorithm\n",
    "\n",
    "Before we can apply PCA, there is a data pre-processing step we must perform:\n",
    "\n",
    "‚ÄúCovariance‚Äù indicates the direction of the linear relationship between variables. ‚ÄúCorrelation‚Äù on the other hand measures both the strength and direction of the linear relationship between two variables. \n",
    "\n",
    "**Use the correlation matrix  when within-variable range and scale widely differs, and use the covariance matrix  to preserve variance if the range and scale of variables is similar or in the same units of measure.**\n",
    "\n",
    "\n",
    "**Data preprocessing**\n",
    "\n",
    "- Given training set: $x^{(1)},x^{(2)},\\dots,x^{(m)}$\n",
    "- Preprocess (feature scaling/mean normalization):\n",
    "    - $\\mu_j = \\frac{1}{m}\\sum_{i=1}^{m}x_{j}^{(i)}$\n",
    "- Replace each $x_j^{(i)}$ with $x_j^{(i)} - \\mu_j$\n",
    "- If different features are on different scales , scale features to have comparable range of values.\n",
    "    - $x_j^{(i)} = \\dfrac{x_j^{(i)} - \\mu_j}{s_j}$\n",
    "\n",
    "\n",
    "1. **Compute Covariance Matrix**\n",
    "\n",
    "$$\\Sigma = \\dfrac{1}{m}\\sum^m_{i=1}(x^{(i)})(x^{(i)})^T$$\n",
    "\n",
    "- Covariance matrix is denoted by capital sigma, do not confuse with symbol for summation\n",
    "- Note that $x^{(i)}$ is an n√ó1 vector, ($(x^{(i)})^T$ is an 1√ón vector and X is a m√ón matrix (row-wise stored examples). The product of those will be an n√ón matrix, which are the dimensions of $\\Sigma$.\n",
    "\n",
    "Percentage of variance retained is given by :\n",
    " $\\displaystyle\\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}^nS_{ii}}$\n",
    "\n",
    "\n",
    "2. compute Singular Value Decomposition (decompose into the dot product of three simple matrices):\n",
    "        * a rotation matrix $U$ (an $m \\times m$ orthogonal matrix)\n",
    "        * a scaling & projecting matrix $\\Sigma$ (an $m \\times n$ diagonal matrix)\n",
    "        * and another rotation matrix $V^T$ (an $n \\times n$ orthogonal matrix)\n",
    "    - extract first two unit vectors that define first two Principal Components from $V^T$\n",
    "    - Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components\n",
    "        - Selecting this hyperplane ensures that the projection will preserve as much variance as possible.\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$z = \\begin{bmatrix}\\vert & \\vert & & \\vert \\\\ u^{(1)} & u^{(2)} & \\dots & u^{(k)}\\\\ \\vert & \\vert & & \\vert \\end{bmatrix}^T x=\\begin{bmatrix} \\text{---} & (u^{(1)})^T & \\mbox{---}\\\\\\text{---} & (u^{(2)})^T & \\mbox{---}\\\\ & \\vdots & \\\\ \\text{---} & (u^{(k)})^T & \\mbox{---}\\end{bmatrix}x$$\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]\n",
    "```\n",
    "\n",
    "**Project the training set onto plane defined by first two principal components**\n",
    "\n",
    "```python\n",
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)\n",
    "```\n",
    "\n",
    "\n",
    "## Reconstruction from compressed Representation\n",
    "\n",
    "\n",
    "## Choosing the Number of Principal Components\n",
    "\n",
    "How do we choose k, also called the number of principal components? Recall that k is the dimension we are reducing to.\n",
    "\n",
    "One way to choose k is by using the following formula:\n",
    "\n",
    "- Given the average squared projection error: $\\dfrac{1}{m}\\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2$\n",
    "\n",
    "- Also given the total variation in the data: $\\dfrac{1}{m}\\sum^m_{i=1}||x^{(i)}||^2$\n",
    "\n",
    "- Choose k to be the smallest value such that: $\\dfrac{\\dfrac{1}{m}\\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\\dfrac{1}{m}\\sum^m_{i=1}||x^{(i)}||^2} \\leq 0.01$\n",
    "\n",
    "\n",
    "In other words, the squared projection error divided by the total variation should be less than one percent, so that **99% of the variance is retained.**\n",
    "\n",
    "- Use SVD to obtain matrix $U$\n",
    "    - a rotation matrix ùëà (an ùëö√óùëö orthogonal matrix)\n",
    "- check for 99% of retained variance using the U matrix as follows:\n",
    "    -  $\\displaystyle\\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}^nS_{ii}} \\geq 0.99$\n",
    "    \n",
    "## Advice for Applying PCA\n",
    "\n",
    "\n",
    "The most common use of PCA is to speed up supervised learning.\n",
    "\n",
    "Given a training set with a large number of features (e.g. $x^{(1)},\\dots,x^{(m)} \\in \\mathbb{R}^{10000}$ ) we can use PCA to reduce the number of features in each example of the training set (e.g. $z^{(1)},\\dots,z^{(m)} \\in \\mathbb{R}^{1000}$).\n",
    "\n",
    "Note that we should define the PCA reduction from $x^{(i)}$ to $z^{(i)}$ only on the training set and not on the cross-validation or test sets. You can apply the mapping $z^{(i)}$ to your cross-validation and test sets after it is defined on the training set.\n",
    "\n",
    "Applications:\n",
    "- compressions\n",
    "    - reduce space of data \n",
    "    - speed up algorithm\n",
    "- visualization of data\n",
    "    - choose k=2 or k=3\n",
    "    \n",
    "Bad use of PCA: trying to prevent overfitting. We might think that reducing the features with PCA would be an effective way to address overfitting. It might work, but is not recommended because it does not consider the values of our results y. Using just regularization will be at least as effective.\n",
    "\n",
    "Don't assume you need to do PCA. Try your full machine learning algorithm without PCA first. Then use PCA if you find that you need it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
