{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence\n",
    "\n",
    "**Indepent Events**\n",
    "\n",
    "$A$ and $B$ are independent if knowing whether $A$ occured gives no information abouth whether $B$ occured . More formally , $A$ and $B$(which have nonzero probability) are independent if and only if one of the following statements hold:\n",
    "\n",
    "$$P(A\\cap B) = P(A)P(B)$$\n",
    "\n",
    "**!! only when $A$ and $B$ are independent,otherwise $A\\subset B$!!**\n",
    "\n",
    "$$P(A\\mid B) = P(A)$$\n",
    "\n",
    "$$P(B\\mid A) = P(B)$$\n",
    "\n",
    "**Conditional independence**\n",
    "\n",
    "$A$ and $B$ are conditionally independent given $C$ if $P(A\\cap B\\mid C) =P(A\\mid C)P(B\\mid C)$\n",
    "\n",
    "**Conditional independence does not imply independence, and independence does not imply conditional independence.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De Morgan`s Law\n",
    "\n",
    "useful identity  that can make calculating probabilities of unions easier by relating them to intersections, and vice versa\n",
    "Analogous results hold with more then two sets\n",
    "\n",
    "$$(A_{1}\\cup A_{2}\\cup \\ldots \\cup A_{n})^{c}=A_{1}^{c}\\cap A_{2}^{c}\\cap \\ldots \\cap A_{n}^{c}$$\n",
    "\n",
    "$$(A_{1}\\cap A_{2}\\cap \\ldots  \\cap A_{n})^{c}=A_{1}^{c}\\cup A_{2}^{c}\\cup \\ldots \\cup A_{n}^{c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint, Marginal and Conditional \n",
    "\n",
    "**Joint Probability** $P(A\\cap B)$ or $P(A,B)$ - Probability of $A$ and $B$\n",
    "\n",
    "**Marginal (Uncoditional) Probability** $P(A)$ - Probability of A\n",
    "\n",
    "**Conditional Probability** - $P(A\\mid B) = P(A,B)/P(B)$ - Probability of $A$ given that $B$ occured\n",
    "\n",
    "**Conditional Probability *is* Probability** - $P(A\\mid B)$ is a probability function for any fixed $B$. Any theorem that holds for probability also holds for conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of an Intersections or Union\n",
    "\n",
    "**Intersections via conditioning** \n",
    "\n",
    "$$P(A,B) = P(A)P(B\\mid A)$$\n",
    "$$P(A,B,C) = P(A)P(B\\mid A)P(C\\mid A,B)$$\n",
    "\n",
    "**Unions via Inclusion-Exclusion**\n",
    "\n",
    "$$P(A\\cup B) = P(A)+P(B)-P(A\\cap B)$$\n",
    "\n",
    "$$P(A\\cup B\\cup C) = P(A)+P(B)+P(C) - P(A\\cap B)- P(A\\cap C) - P(B\\cap C)+P(A\\cap B\\cap C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "\n",
    "if $A$ and $B$ are events with $P(B)>0$, then the *conditional probability* of $A$ given $B$, denoted by $P\\:(A\\mid B)$ is defined as\n",
    "\n",
    "$$P(A\\mid B) = \\frac{P(A\\cap B)}{P(B)}$$\n",
    "\n",
    "Here  $A$ is the event which uncertainty we want to update , and $B$ is the evidence we observe (or we want to treat as given). We call  $P(A)$ the *prior* probability of $A$ and $P(A\\mid B) the *posterior* probability of $A$ ('prior' means before updating based on the evidence, and the 'posterior ' means after updating based on the evidence).\n",
    "\n",
    "$P(A\\mid B)$ is the probability of $A$ given evidence $B$,      **NOT** THE PROBABILITY OF SOME ENTITY CALLED $A\\mid B$\n",
    "\n",
    "\n",
    "When we calculate conditional probabilities, we do not assume  whether one event causes other, **we are considering what information observing one event provides about another event.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"prob_img/cond_intuituion_pebble_world.PNG\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 2.3.1\n",
    "\n",
    "For any events $A$ and $B$ with positive probabilities\n",
    "\n",
    "$$P\\:(A\\cap B) = P(B)\\:P(A\\mid B)= P(A)\\:P(B\\mid A)$$\n",
    "\n",
    "- just slightly differently written definition of conditional probability\n",
    "  - but can be useful because it can help us find $P\\:(A\\cap B)$ without going back to definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 2.3.2**\n",
    "\n",
    "For any events $A_{1},\\ldots,A_{n}$ with positive probabilities\n",
    "\n",
    "$$P(A_{1},A_{2},\\ldots,A_{n})= P(A_{1})P(A_{2}\\mid A_{1})P({A_{3}\\mid A_{1},A_{2}})\\ldots P(A_{n}\\mid A_{1},\\ldots,A_{n-1}) $$\n",
    "\n",
    "The commas denote intersections. For example , $P(A_{3}\\mid A_{1},A_{2})$ is the probability that $A_{3}$ occurs, given that both $A_{1}$ and $A_{2}$ occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 2.3.3 (Bayes Rule)**\n",
    "\n",
    "\n",
    "$$P(A\\mid B) = \\frac{P(B\\mid A)P(A)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Odds of an event A**\n",
    "\n",
    "$odds(A)=P(A)/P(A^{c})$\n",
    "\n",
    "for example, if $P(A)=2/3$, we say the odds in favor of $A$ are 2 to 1(2:1)\n",
    "\n",
    "To convert from odds back to probability:\n",
    "\n",
    "$P(A)=odds(A)/(1+odds(A))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 2.3.5**\n",
    "\n",
    "**Odds form of Bayes rule**\n",
    "\n",
    "For any events $A$ and $B$ with positive probabilities, the odds of $A$ after conditioning on $B$ are:\n",
    "\n",
    "$$\\frac{P(A\\mid B)}{P(A^{c}\\mid B)}=\\frac{P(B\\mid A)}{P(B\\mid A^{c})}\\frac{P(A)}{P(A^{c})}$$\n",
    "\n",
    "In words this says, that the *posterior* *odds* $P(A\\mid B)/P(A^{c}\\mid B)$ are equal to *prior* *odds* $P(A)/P(A^{c})$ times the factor $P(B\\mid A)/P(B\\mid A^{c})$,which is known in statitistics as *likelihood* *ratio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 2.3.4 (Law of total probability (LOTP))**\n",
    "\n",
    "\n",
    "$$P(B)=\\sum_{i=1}^{n}P(B\\mid A_{i})P(A_{i})$$\n",
    "\n",
    "$$P(B)=\\sum_{i} P(B \\cap A_i)=\\sum_{i} P(B | A_i) P(A_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "Since the $A_{i}$ form the partition of $S$, we can decompose $B$ as\n",
    "\n",
    "$$P(B)=P(B\\cap A_{1})\\cup P(B\\cap A_{2})\\cup \\ldots \\cup P(B\\cap A_{n})$$\n",
    "\n",
    "<img src='prob_img/LOTP.PNG' />\n",
    "\n",
    "**The law of total probability tells us that to get the unconditional probability of , we can divide the sample space into disjoint slices , find the conditional probability of within each of the slices, then take a weighted sum of the conditional probabilities, where the weights are the probabilities.** \n",
    "\n",
    "**The choice of how to divide up the sample space is crucial: a well-chosen partition will reduce a complicated problem into simpler pieces, whereas a poorly chosen partition will only exacerbate our problems, requiring us to calculate difficult probabilities instead of just one!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thinking Conditionally is condition for thinking\n",
    "\n",
    "**How to solve a problem ?**\n",
    "\n",
    "1.) Try simple and extreme cases\n",
    "\n",
    "2.) Break up  problem into simpler pieces (**LOTP**)\n",
    "\n",
    "**Hazards:**\n",
    "\n",
    "- Prosecutors fallacy:\n",
    "  - $P($Innocence$\\mid\\:$Evidence$)$ vs $P($Evidence$\\mid\\:$Innocence$)$\n",
    "- Prior vs Posterior:\n",
    "  - Prior = before we have evidence\n",
    "  - Posterior = after we have evidence\n",
    "- Confusing independence vs conditional indpendence\n",
    "  - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional probabilities are probabilities \n",
    "\n",
    "When we condition on arbitrary event $E$, we update our beliefs to be consistent with, effectively putting ourselves in a universe where we know that $E$ occured.\n",
    "\n",
    "Within our new universe,however, laws of probability operates just as before.\n",
    "\n",
    "**Conditional probability satisfies all the properties of probability!**\n",
    "\n",
    "Therefore,any of the results we have derived about probability are still valid if we replace all uncoditional probabilities with probabilities conditional on $E$. In particular:\n",
    "\n",
    "- Conditional probabilities are between 0 and 1\n",
    "- $P(S\\mid E) = 1$,$P(\\varnothing\\mid E)= 0$\n",
    "- if $A_{1},A_{2},\\ldots$ are disjoint,then $P(\\bigcup_{j=1}^{\\infty}A_{j}\\mid E)=\\sum_{j=1}^{\\infty}P(A_{j}\\mid E)$\n",
    "- $P(A^{c}\\mid E) = 1 - P(A\\mid E)$\n",
    "- Inclusion-Exclusion : $P(A\\cup B\\mid E)=P(A\\mid E)+P(B\\mid E) - P(A\\cap B\\mid E)$\n",
    "\n",
    "When we write $P(A\\mid E)$, it does **not** mean that $A\\mid E$ is an event and we are taking it`s probability; $A\\mid E$ is not event!.\n",
    "\n",
    "Rather, $P(\\cdotp \\mid E)$ is a probability function  which assigns probabilities in accordance with the knowledge that E has occured, and $P(\\cdotp)$ is a different probability function  which assigns probabilities without regard for whether $E$ has occured or not. When we take an event $A$ and plug it into the $P(\\cdotp)$ function , we`ll get another number, $P(A\\mid E)$, which incorporates the information(if any) provided by knowing that $E$ occured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes Rule with extra conditioning**\n",
    "\n",
    "Provided that $P(A\\cap E) > 0$ and $P(B\\cap E)> 0$, we have\n",
    "\n",
    "$$P(A\\mid B,E) = \\frac{P(B\\mid A,E)\\:P(A\\mid E)}{P(B\\mid E)}$$\n",
    "\n",
    "\n",
    "**LOTP with extra conditioning**\n",
    "\n",
    "Let $A_{1},\\ldots,A_{n}$ be a partition of S. Provided that $P(A_{i}\\cap E)> 0$ for all *i*, we have :\n",
    "\n",
    "$$P(B\\mid E)=\\sum_{i=1}^{n}P(B\\mid A_{i},E)\\:P(A_{i}\\mid E)$$\n",
    "\n",
    "#### Risks\n",
    "\n",
    "- 1) Confusing $P(A\\mid B)$ with $P(B\\mid A)$ \n",
    "  - **Prosecutor fallacy**\n",
    "    - we want $P($ Innocence$\\mid$ Evidence$)$, not  $P($ Evidence$\\mid$ Innocence$)$\n",
    "- 2) confusing $P(A)$ *prior* with $P(A\\mid B)$ *posterior*\n",
    "  - $P(A\\mid A)= 1$\n",
    "    - $P(A)\\neq 1$\n",
    "- 3) Confusing independence with conditional independence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence of Events\n",
    "\n",
    "Events $A$ and $B$ are independent if\n",
    "\n",
    "$$P(A\\cap B) = P(A)P(B)$$\n",
    "\n",
    "if $P(A)>0$ and $P(B)>0$, then this is equivalent to \n",
    "\n",
    "$$P(A\\mid B) = P(A)$$\n",
    "\n",
    "an also equivalent to $P(B\\mid A) = P(B)$\n",
    "\n",
    "- i.e., two events are independent if we can obtain probability of their intersection by multiplying their individual probabilities\n",
    " - alternatively $A$ and $B$ are indepent if learning that $B$ occured gives us no information that would change probability of $A$ occuring( and vice versa)\n",
    "   - independence is symmetric (if $A$ is indepent of $B$, then $B$ is indepent of $A$\n",
    "\n",
    "\n",
    "<mark >\n",
    "Independence is completely different from disjointness. If A and B are disjoint, then $P(A\\cap B)=0$, so disjoint events can be independent only if $P(A)= 0$ or $P(B)=0$. Knowing that $A$ occurs,tells us that $B$ definitely did not occur , so $A$ clearly  conveys information $B$\n",
    "</mark>\n",
    "\n",
    "#### Independence of three events\n",
    "- Events $A$,$B$ and $C$ are said to be independent if all of the following equations hold:\n",
    "$$P(A\\cap B) =P(A)P(B)$$\n",
    "$$P(A\\cap C) =P(A)P(C)$$\n",
    "$$P(B\\cap C) =P(B)P(C)$$\n",
    "$$P(A\\cap B\\cap C) =P(A)P(B)P(C)$$\n",
    "\n",
    "\n",
    "  - if the first three conditions hold, we say that $A$,$B$ and $C$ are *pairwise* *independent*\n",
    "    - *pairwise* *independence * does not mean independence\n",
    "  - Example:\n",
    "    - Two fair, independent coin tosses: \n",
    "      - $A= \\{H\\}$ - firt toss is Heads\n",
    "      - $B=\\{..,H\\}$ - second toss is Heads\n",
    "      - $C=\\{HH,TT\\}$ - both tosses have same result\n",
    "      - $A$,$B$,$C$ are pairwise independent but not indepent since, $P(A\\cap B\\cap C)=1/4$, while $P(A)P(B)P(C)=1/8$\n",
    "          - **knowing about just $A$ or just $B$ , tell us nothing about C, but knowing what happened with both $A$ and $B$ ,gives us information about $C$**\n",
    "          \n",
    "#### Conditional independence\n",
    "\n",
    "Events $A$ and $B$ are said to be *conditionally* *independent* given event $E$ if $P(A\\cap B\\mid E) = P(A\\mid E)P(B\\mid E)$\n",
    "\n",
    "- **Example: Does conditional independence give C, imply independence ?**\n",
    "  - chess player of unknown strength\n",
    "    - even though games are seemingly independent\n",
    "      - after winning first 5 games in a row, evidence of this winning streak is relevant for predicting the later outcomes\n",
    "    - Independence would mean that the earlier games give you no information whatsoever that helps you predict the outcomes of the later games\n",
    "    \n",
    "    \n",
    "- **Example : Conditional independence does not imply independence**\n",
    "  - two coins, we dont know which one is chosen\n",
    "    - fair coin\n",
    "    - biased coin - probability 3/4 heads\n",
    "  - conditioning on choosing the fair coin\n",
    "    - tosses are independent, each toss has 1/2 probability\n",
    "  - conditioning on choosing the biased coin\n",
    "    - tosses are independent, each toss has 3/4 probability\n",
    "  - coin tosses are not unconditionally independent , because if we dont know which coin we have chosen , then observing the sequence of tosses gives us information whether  we have fair coin, or biased coin in our hand.\n",
    "    - this helps us to predict the outcome of future tosses\n",
    "  - Let :\n",
    "    - $F$= {Fair coin}\n",
    "    - $A_{1}$ = {First toss is Heads}\n",
    "    - $A_{2}$ = {Second toss is Heads}\n",
    "      - Conditioning on $F$,$A_{1}$,$A_{2}$ are independent, but $A_{1}$,$A_{2}$ are unconditionally independent because $A_{1}$ provides information about $A_{2}$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
