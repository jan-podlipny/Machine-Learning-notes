{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Instead of predicting exactly 0 or 1, **logistic** **regression** generates a probability, a value between 0 and 1,exclusive.\n",
    "- Consider logistic regression model for spam detection.\n",
    "  - If the model infers a value of 0.932, on a particular email message, it implies a 93.2% probability of email message being spam.\n",
    "    - More precisely , it means that in the limit of infinite training examples, the set of  examples for which the model predicts 0.932 will actually be spam 93.2% of the time and the remaining 6.8% will not.\n",
    "    \n",
    "- **probability** **estimates** are calibrated\n",
    "  - for example,$P$(house will sell)$*$price = expected outcome\n",
    "- Regularization is extremely important for logistic regression\n",
    "  - asymptotes of $log$**loss**\n",
    "  - it will keep trying to drive loss to 0 in high dimensions\n",
    "- two strategies are especially useful:\n",
    "  - $L_{2}$**regularization**(aka $L_{2}$ weight decay) - penalizes huge weights\n",
    "  - **Early** **stopping** - limiting training steps or learning rate\n",
    "\n",
    "- Linear logistic regression is extremely efficient.\n",
    "  - very fast training and prediction times\n",
    "  - Short/wide models use a lot of RAM\n",
    "  - of we need non-linearities, we can get them by adding in **feature** **cross** **products**\n",
    "\n",
    "### Logistic Regression: Calculating a Probability \n",
    "\n",
    "Many problems require a probability estimate as output. Logistic regression is an extremely efficient mechanism for calculating probabilities. Practically speaking, you can use the returned probability in either of the following two ways:\n",
    "  - \"As is\"\n",
    "  - Converted to a binary category\n",
    "  \n",
    "Let's consider how we might use the probability \"as is.\" Suppose we create a logistic regression model to predict the probability that a dog will bark during the middle of the night. We'll call that probability:\n",
    "\n",
    "$P(bark\\mid night)$\n",
    "\n",
    "If the logistic regression model predicts a $P(bark\\mid night)$ of 0.05, then over a year, the dog's owners should be startled awake approximately 18 times:\n",
    "\n",
    "startled = $P(bark\\mid night)$ * nights\n",
    "\n",
    "18 ~= 0.05 * 365\n",
    "\n",
    "In many cases, you'll map the logistic regression output into the solution to a binary classification problem, in which the goal is to correctly predict one of two possible labels (e.g., \"spam\" or \"not spam\").\n",
    "\n",
    "You might be wondering how a logistic regression model can ensure output that always falls between 0 and 1. As it happens, a sigmoid function, defined as follows, produces output having those same characteristics:\n",
    "\n",
    "$$y = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "The sigmoid function yields the following plot:\n",
    "\n",
    "<img src=\"google_ml_img\\sigmoidfunction.png\">\n",
    "\n",
    "\n",
    "If z represents the output of the linear layer of a model trained with logistic regression, then sigmoid(z) will yield a value (a probability) between 0 and 1. In mathematical terms:\n",
    "\n",
    "$$y' = \\frac{1}{1 + e^{-(z)}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- y' is the output of the logistic regression model for a particular example.\n",
    "- z is b + w1x1 + w2x2 + ... wNxN\n",
    "  - The w values are the model's learned weights and bias.\n",
    "  - The x values are the feature values for a particular example.\n",
    "\n",
    "Note that z is also referred to as the log-odds because the inverse of the sigmoid states that z can be defined as the log of the probability of the \"1\" label (e.g., \"dog barks\") divided by the probability of the \"0\" label (e.g., \"dog doesn't bark\")\n",
    "\n",
    "$$z = log(\\frac{y}{1-y})$$\n",
    "\n",
    "**sigmoid function with ML labels**\n",
    "\n",
    "<img src=\"google_ml_img\\LogisticRegressionOutput.svg\">\n",
    "\n",
    "### Loss function for Logistic Regression\n",
    "\n",
    "The loss function for linear regression is squared loss. The loss function for logistic regression is **Log Loss**, which is defined as follows:\n",
    "\n",
    "$$\\text{Log Loss} = \\sum_{(x,y)\\in D} -y\\log(y') - (1 - y)\\log(1 - y')$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $(x,y)\\in D$ is the data set containing many labeled examples, which are $(x,y)$ pairs\n",
    "- $y$ is the label in a labeled example. Since this is logistic regression, every value of $y$ must either be 0 or 1. \n",
    "- $y'$  is the predicted value (somewhere between 0 and 1), given the set of features in $x$\n",
    "\n",
    "The equation for Log Loss is closely related to Shannon's Entropy measure from Information Theory. It is also the negative logarithm of the likelihood function, assuming a Bernoulli distribution of . Indeed, minimizing the loss function yields a maximum likelihood estimate.\n",
    "\n",
    "\n",
    "#### Regularization in Logistic Regression\n",
    "\n",
    "Regularization is extremely important in logistic regression modeling. Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in high dimensions. Consequently, most logistic regression models use one of the following two strategies to dampen model complexity:\n",
    "  - L2 regularization.\n",
    "  - Early stopping, that is, limiting the number of training steps or the learning rate.\n",
    "  - there is also third strategy, L1 regularization\n",
    "\n",
    "Imagine that you assign a unique id to each example, and map each id to its own feature. If you don't specify a regularization function, the model will become completely overfit. That's because the model would try to drive loss to zero on all examples and never get there, driving the weights for each indicator feature to +infinity or -infinity. This can happen in high dimensional data with feature crosses, when thereâ€™s a huge mass of rare crosses that happen only on one example each.\n",
    "\n",
    "Fortunately, using L2 or early stopping will prevent this problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
