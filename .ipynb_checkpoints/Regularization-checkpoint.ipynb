{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "\n",
    "- when hypothesis function is not fitting data very well\n",
    "  - underfit or \"high bias\"\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "- if we have too many features, the learned hypothesis function may fit the training set very well ($J(\\theta) = \\frac{1}{2m}\\sum_{i}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^{2} \\approx 0$ ), but **fail to** **generalize** to new examples (predict prices on new examples)\n",
    "  - overfitting or alternatively \"High Variance\"\n",
    "\n",
    "### Addresing overfitting\n",
    "\n",
    "- 1) Reduce number of features\n",
    "  - Manually select which features to keep\n",
    "  - Model selection algorithm\n",
    "- 2) Regularization\n",
    "  - Keep all the features, but reduce magnitude/values of parameter $\\theta_{j}$\n",
    "  - Works well when we have a lot of features, each of which contributes a bit to predictin $y$\n",
    "\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "- one way to regularize cost function is to penalize and make some $\\theta$ parameter small\n",
    "  - lets make $\\theta_{3}$ and $\\theta_{4}$ small\n",
    "<font size = \"4\" > $$\\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^i)-y^i)^{2}+1000\\: \\theta_{3}^{2}+1000\\: \\theta_{4}^{2}$$</font>\n",
    "  - only way to make new cost function small is when  $\\theta_{3}$ and $\\theta_{4}$ are small\n",
    "    - this way we will end up with $\\theta_{3}\\approx 0$ and $\\theta_{4}\\approx 0$\n",
    "\n",
    "### Basic intuition behind Regularization\n",
    "\n",
    "- Small values for parameters $\\theta_{0},\\theta_{1},\\ldots \\theta_{n}$\n",
    "  - \"simpler\" hypothesis\n",
    "  - less prone to overfitting\n",
    "  - if all $\\theta$ parameters are smaller is also helpful ( need to implement by yourself to understand)\n",
    "<font size = \"4\" >\n",
    "$$J(\\theta)=[\\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^i)-y^i)^{2} + \\lambda \\sum_{i}^{n}\\theta_{J}^{2}]$$</font>\n",
    "\n",
    "- where $\\lambda$ is regularization parameter\n",
    "  - $\\lambda$ controls trade-off between fitting the training set well, and goal of keeping the parameters small and therefore keeping hypothesis relatively simple\n",
    "  - very large $\\lambda$ (for example $10^{10}$) leads to underfitting\n",
    "\n",
    "- in practice $\\theta_{0}$ is usually not penalized (left at $\\theta_{0}=1$)\n",
    "  - if penalized, it makes little difference to results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression\n",
    "\n",
    "<font size = \"4\" >\n",
    "$$J(\\theta)=\\bigg[\\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^i)-y^i)^{2} + \\lambda \\sum_{i}^{n}\\theta_{J}^{2}\\bigg]$$\n",
    "</font>\n",
    "\n",
    "- we would like to find parameters of $\\theta$ that minimizes cost function $\\underset{\\theta}{\\text{min}}\\:J(\\theta)$\n",
    "\n",
    "####  Regularized Gradient Descent\n",
    "<font size = \"4\" >\n",
    "$$\\theta_{j}:= \\theta_{j}-\\alpha \\bigg[\\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^i)-y^i)x^{j}+\\frac{\\lambda}{m}\\theta_{j}\\bigg]$$\n",
    "</font><font size = \"6\" >    \n",
    "- term in brackets [  ] is $\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta)$ regularized   \n",
    "</font>\n",
    "\n",
    "- Regularized Gradient Descent can also be written as :\n",
    "<font size = \"4\" >\n",
    "$$\\theta_{j}:= \\theta_{j}(1-\\alpha\\frac{\\lambda}{m})-\\alpha \\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^i)-y^i)x^{j}$$</font>\n",
    "\n",
    "\n",
    "#### Regularized Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression\n",
    "\n",
    "####  Logistic Regression\n",
    "\n",
    "$$J(\\theta)=-\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}\\log h_{\\theta}(x^{(i}))+(1-y^{i})\\log(1-h_{\\theta}(x^{(i)}))]$$\n",
    "\n",
    "\n",
    "#### Regularized Logistic Regression\n",
    "\n",
    "##### Regularization Term\n",
    "$$\\frac{\\lambda}{2m}\\sum_{j=i}^{n}\\theta_{j}^{2}$$\n",
    "\n",
    "#### Logistic Regression + Regularization Term\n",
    "$$J(\\theta)=-\\frac{1}{m}\\big[\\sum_{i=1}^{m}y^{(i)}\\log h_{\\theta}(x^{(i}))+(1-y^{i})\\log(1-h_{\\theta}(x^{(i)}))\\big]+\\frac{\\lambda}{2m}\\sum_{j=i}^{n}\\theta_{j}^{2}$$\n",
    "\n",
    "\n",
    "### Gradient Descent for Regularized Logistic regression\n",
    "\n",
    "- we want too keep $\\theta_{0}=1$ (non-regularized)\n",
    "  - 2 functions , one for $\\theta_{0}$, second for rest of $\\theta$\n",
    "$$\\theta_{0} := \\theta{j}-\\alpha\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})\\:x_{0}^{(i)}$$\n",
    "\n",
    "$$\\theta_{j} := \\theta{j}-\\alpha\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})\\:x_{j}^{(i)}+\\frac{\\lambda}{m}\\theta{j}$$\n",
    "\n",
    "$$j \\in (1,2,\\ldots,n)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
