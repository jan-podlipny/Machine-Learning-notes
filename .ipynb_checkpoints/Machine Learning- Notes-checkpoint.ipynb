{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Machine Learning\n",
    "\n",
    "- Based on type of training (with or without human supervision)\n",
    "   - supervised,unsupervised,semisupervised and Reinforcement learning\n",
    "- Whether or not they can learn incrementally on the fly\n",
    "   - online vs batch learning\n",
    "- If  they work by comparing new data points to known data points, or instead detect patterns in training data and build predictive model\n",
    "   - instance-based vs model-based learning\n",
    "\n",
    "These criteria are not exclusive; you can combine them in any way you like. For example, a state-of-the art\n",
    "spam filter may learn on the fly using a deep neural network model trained using examples of spam and\n",
    "ham; this makes it an online, model-based, supervised learning system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "\n",
    "In supervised learning, training data includes desired solutions(labels)\n",
    "\n",
    "Types of task:\n",
    "\n",
    "- Classification:\n",
    "  - \n",
    "- Regression:\n",
    "  - predicting a *target* numeric value,such as a price of house, given set of *features*(size,accesory,garden size,etc...) called predictors\n",
    "  - some regression algorithms can be used for classification\n",
    "    - *Logistic Regression* is often used for classification as it can output a value that corresponds to the probability of belonging to a given class(e.g. 20% chance of being spam)\n",
    "\n",
    "*Most important algorithms*\n",
    "\n",
    "- k-Nearest Neighbors\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Support Vector Machine\n",
    "- Decision Trees and Random Forest\n",
    "- Neural Networks ( can also be semisupervised or unsupervised)\n",
    "\n",
    "\n",
    "#### Unsupervised Learning\n",
    "\n",
    "System tries to learn without teacher.\n",
    "\n",
    "*Most important algorithms*\n",
    "\n",
    "- Clustering\n",
    "  - k-Means\n",
    "  - Hiearchical Cluster Analysis (HCA)\n",
    "  - Expectation Maximization\n",
    "- Visualization and dimensionality reduction\n",
    "  - Principal Component Analysis (PCA)\n",
    "  - Kernel PCA\n",
    "  - Locally-Linear Embedding(LLE)\n",
    "  - t-distributed Stochastic Neighbor Embedding(t-SNE)\n",
    "- Association rule learning\n",
    "  - Apriori\n",
    "  - Eclat\n",
    "  \n",
    "in ML *attribute* is a data type ('Car Mileage'), *feature* usually means attribute+ it`s value ('Car Mileague=15000km)\n",
    "\n",
    "**Dimensionality** **reduction** - goal is to simplify data without losing too much information.\n",
    "  - for example, merge several correlated features into one ( car mileage may be very correlated with its age, so dimensionality reduction algorithm will merge them into one feature that represents car`s wear and tear). This is called *feature* *extraction*\n",
    "  \n",
    "**Anomaly** **Detection**\n",
    "  - detecting unusual credit card transactions,catching manufacturing defects, or automatically removing outliers from dataset before feeding it to another learning algorithm\n",
    "**Association** **rule** **learning**\n",
    "  - sif through large amounts of data and discover interesting relations between attributes\n",
    "    - for example, running association rule on your sales logs may reveal that people who purchase barbecue sauce  and potate chips also tend to buy steak, thus you may want to place these items close together\n",
    "    \n",
    "#### Semisupervised learning\n",
    "\n",
    "#### Reinforcement Learning\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch and Online Learning\n",
    "\n",
    "#### Batch learning\n",
    "- System is trained using all available data\n",
    "  - takes a lot of computing resources\n",
    "    - typically done offline\n",
    "  - after training, system is put to production and runs without learning, just applies what it has learned. \n",
    "  - This is called **Offline** **Learning**\n",
    "    - after some time, it needs to be trained again on new and old data\n",
    "    - then stop old system, and replace it with new one\n",
    "      - this process can be automated \n",
    "\n",
    "\n",
    "#### Online(Incremental) Learning\n",
    "\n",
    "<img src= 'notes_img\\online_learning.png'>\n",
    "\n",
    "- training is done incrementally by feeding its data instances sequentially\n",
    "  - either individually, or by *mini-batches**\n",
    "- great for systems that receives data as continuous flow and need to adapt to change rapidly and/or autonomously\n",
    "- Also works well if you have limited resources\n",
    "  - once an online learning system has learned about new datainstances, it can discard them\n",
    "    - still good idea to save them, to be able to roll back to previous state and 'replay' data\n",
    "- *out-of-core* *learning*\n",
    "  - used if data can not fit in one machine main memory\n",
    "    - algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data\n",
    "- *Learning* *rate*\n",
    "  - tells alg how fast it should adapt to changing data\n",
    "    - high learning rate > system will rapidly adapt to new data\n",
    "      - but quickly forgets old data\n",
    "    - slow learning rate > system will have more inertia(setrvacnost)\n",
    "      - learns more slowly, but less sensitive to noise in data, or nonrepresentative data points\n",
    "  - its important to closely monitor online learning system because bad data will gradually worsen system performance\n",
    "    - in extreme case switch learning off\n",
    "    - if possible monitor input data and react to abnormal data (anomaly detection algorithm)\n",
    "\n",
    "\n",
    "<img src= 'notes_img\\out_of_core.png'>  \n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance-based vs Model-based Learning\n",
    "\n",
    "#### Instance-based\n",
    "- based on measure of similarity\n",
    "  - systems learn examples by heart, and then generalizes to new cases using a similarity measure\n",
    "\n",
    "<img src= 'notes_img\\instance_based.png'>    \n",
    "\n",
    "\n",
    "#### Model-based learning\n",
    "- study data\n",
    "- select model\n",
    "    - train on training data ( learning algorithm searched for the model paramater values that minimizes the cost function )\n",
    "- apply model on new data to make predictions on new cases (*inference*)\n",
    "\n",
    "<img src= 'notes_img\\model_based.png'>   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Challenges of Machine Learning\n",
    "\n",
    "#### Insufficient Quantity of Training Data\n",
    "- for very simple problems , we need thousands of examples\n",
    "- for complex problems (image or speech recognition) we may need millions of examples (unless you can reuse existing model)\n",
    "\n",
    "#### Nonrepresentative Training Data\n",
    "- in order to generalize well, it is crucial that training data are representative of new cases you want to generalize to\n",
    "  - true for both instance and model based learning\n",
    "- **Sampling** **noise**\n",
    "  - non representative data as result of chance (if sample is too small)\n",
    "- **Sampling** **bias**\n",
    "  - even large samples can be nonrepresentative if the sampling method is flawed.\n",
    "#### Poor Quality Data\n",
    "- data full of outliers, errors and noise makes it harder for system to detect patterns\n",
    "  - often is worth to spend time tidying up your data set\n",
    "- if some instances are clearly outliers , it may help to simply discard them, or to try fix errors manually\n",
    "- if some instances are missing a few features(e.g. 5% of your customers did not specify their age)\n",
    "  - ignore this attribute altogether\n",
    "  - ignore these instances\n",
    "  - fill in missing values (e.g. with median age)\n",
    "  - or train model with the feature and one model without feature\n",
    "#### Irrelevant features\n",
    "**Garbage** **In** **Garbage** **Out**\n",
    "\n",
    "- Feature engineering\n",
    "  - process of selecting good features for model\n",
    "  - Feature selection\n",
    "    - selecting the most useful features to train on amongst existing features\n",
    "  - Feature extraction\n",
    "    - combining existing features to produce a more useful one ( dimensionality reduction algorithm can help)\n",
    "  - Creating new features by gathering data\n",
    "\n",
    "#### Overfitting the Training Data \n",
    "- i.e. overgeneralizing\n",
    "- it means that model performs well on training data, but does not generalize well\n",
    "\n",
    "- Complex model can detect subtle patterns in data, but if training set is too noisy, or if it is too small (i.e. sampling noise)\n",
    "the model is likely to detect patterns in noise\n",
    "\n",
    "- **Solutions**\n",
    "  - simplify model by selecting one with fewer parameters (e.g. linear model rather that a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining model\n",
    "    - this is called **Regularization**\n",
    "      - amount of regularization can be controlled by *hyperparameter*\n",
    "        - hyperparameter is a parameter of learning algorithm (not of the model)\n",
    "  - gather more training data\n",
    "  - reduce noise in the training data (e.g. fix data errors and remove outliers)\n",
    "\n",
    "#### Underfitting the Training Data\n",
    "- occurs when model is too simple learn underlying structure of the data \n",
    "- Solutions:\n",
    "  - Selecting a more powerful model, with more paramaters\n",
    "  - Feeding better features to the learning algorithm ( feature engineering)\n",
    "  - Reducing the constraints on the model (e.g., reducing the regularization hyperparameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing and Validating\n",
    "\n",
    "- split data into *training* set and *test* set\n",
    "  - error rate on new cases is called *generalization* *error* and by evaluating your model on test set , you get an estimation error\n",
    "  - common ratio is to use 80% for training  and *hold* *out* 20% for testing (or 70/30)\n",
    "- common pitfall is to measure generalization error multiple times on test set, until you adapt your model and hyperparameters to your test set ( that makes it unlikely to perform well on new data)\n",
    "  - common solution is to have second hold-out set (*validation* *set*)\n",
    "    - train multiple models with various hyperparameters on training set\n",
    "    - select model that perform best on validation set\n",
    "    - run single final test on test set to get an estimate of generalization error\n",
    "- to avoid wasting to much training data in validation sets, technique called **cross-validation** is often used\n",
    "  - training set is split in complementary subsets and each model is trained against different combination of these subsets and validated against remaining parts.\n",
    "  - once a model type and hyperparameters have been selected, a final model is trained using these hyperparameters on the full training set, and the generalized error is measured on the set\n",
    "  \n",
    "#### No Free Lunch Theorem\n",
    "\n",
    "- if you make absolutely no assumptions about data, then there is no reason to prefer one model over any other\n",
    "  - not possible in practice due time constraints\n",
    "    - neccesary to make assumptions and make educated guess with which model to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
